{
  "hash": "0542d6946d9f6398b49bd19d65d40290",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text-mining a Taylor & Francis journal using R\"\nauthor:\n  - name: Pablo Gómez Barreiro\n    orcid: 0000-0002-3140-3326\ndate: \"2023-10-11\"\ncategories: [R, rcrossref, Taylor & Francis, Text-mining, The Strain on Scientific Publishing]\nimage: \"thumbn.jpg\"\ndraft: false\ncomments:\n  utterances: \n    repo: pgomba/pgb_website\n    label: comment\n    theme: github-light\n    issue-term: title\n---\n\n\n\nThis is the second tutorial/blog exploring some of text-mining/web-scraping methods used on the preprint ([The Strain on Scientific Publishing](https://arxiv.org/abs/2309.15884)). Head over to [Text-mining PLOS articles using R](https://pagomba-blog.netlify.app/posts/08_10_23/) to read the first instance. Today, I will be focusing on [Taylor & Francis](https://taylorandfrancis.com/) (T&F)\n\nTo some extent, Taylor & Francis allows web scraping their website. To do so you have to be a subscriber to their content, have a non-commercial purpose and send a request. This can take some time, and, in the meantime, any web scraping attempts will return an **Error 403** (access forbidden). Fortunately, editorial times of scientific articles from T&F are available on [Crossref](https://www.crossref.org), and their API can be accessed with R using the package [`rcrossref`](https://CRAN.R-project.org/package=rcrossref).\n\n### Step 1: Be polite\n\n`rcrossref` encourages their users to identify themselves in every query. This is a straight forward process that is well documented [here](https://github.com/ropensci/rcrossref):\n\n[![rcrossref GitHub repository - screenshot](polite.PNG)](https://github.com/ropensci/rcrossref)\n\nAfter restarting your R session, is time to load all necessary libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rcrossref)\n```\n:::\n\n\n\n### Step 2: Choosing a journal\n\n`rcrossref` is going to need a ISSN number to go fetch data. This number is usually available in the journal description page. For this example I´m going to choose the journal [Plant Ecology & Diversity](https://www.tandfonline.com/journals/tped20). The ISSNs (Print and online version) can be found on its [Journal Information](https://www.tandfonline.com/action/journalInformation?journalCode=tped20) page: 1755-0874 & 1755-1668.\n\nTo get a hint of what we might find using Crossref data on this journal we can take a peak in [Scimago](https://www.scimagojr.com/). If you type in and search the journal name or the ISSNs there is some useful information. In particular, the total number of documents is of interest. The larger the number of available documents is, the longer obtaining the data from `rcrossref` will take (and it can get tedious with LARGE requests).\n\n![Total number of documents for the journal Plant Ecology & Diversity, according to Scimago - screenshot](scimago.PNG){fig-align=\"center\"}\n\n### Step 3: Go fetch!\n\nLet´s build now the data request using R code using the `rcrossref` function `cr_works()`. I´m going to use the online ISSN (1755-1668) and select only publications published in 2015 and after. If you wish to obtain data from all publications just drop `from_pub_date=\"2015-01-01` from the code below. Additionally, we set the parameter `cursor` to not have a limit in number of articles by just adding an unreasonable high number. For more information on how `rcrossref` and `cr_works()` can be used head to their documentation page [here](https://docs.ropensci.org/rcrossref/index.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njournal_info<-cr_works(filter=c(issn=\"1755-1668\",from_pub_date=\"2015-01-01\"),cursor = \"*\",cursor_max = 500000)\n```\n:::\n\n\n\nOnce the code is running, it might take some minutes for the data to come back. When finished, the object `journal_info` will be a large list with 3 elements. Let´s have a look to what is inside this object on **Step 4**.\n\n![journal_info is a large list with 3 elements](journal_info.PNG){fig-align=\"center\"}\n\n### Step4: Unboxing \"journal_info\"\n\nNow that we have `rcrossref` output (`journal_info`), lets have a look to the elements within. The first element is `meta`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njournal_info$meta[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  total_results search_terms start_index items_per_page\n1           338           NA           0             20\n```\n\n\n:::\n:::\n\n\n\nThis is a data frame incicating we have obtained all the Crossref data available for 309 scientific publications.\n\nThis information is contained in the element `data`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(journal_info$data,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 34\n   alternative.id              container.title created deposited published.print\n   <chr>                       <chr>           <chr>   <chr>     <chr>          \n 1 10.1080/17550874.2018.1540… Plant Ecology … 2018-1… 2020-09-… 2018-07-04     \n 2 10.1080/17550874.2018.1471… Plant Ecology … 2018-0… 2020-09-… 2018-03-04     \n 3 10.1080/17550874.2018.1507… Plant Ecology … 2018-0… 2020-09-… 2018-05-04     \n 4 10.1080/17550874.2015.1049… Plant Ecology … 2015-0… 2016-12-… 2015-07-04     \n 5 10.1080/17550874.2024.2422… Plant Ecology … 2024-1… 2025-01-… 2024-07-03     \n 6 10.1080/17550874.2018.1549… Plant Ecology … 2018-1… 2024-07-… 2019-11-02     \n 7 10.1080/17550874.2019.1641… Plant Ecology … 2019-0… 2024-07-… 2019-09-03     \n 8 10.1080/17550874.2015.1123… Plant Ecology … 2016-0… 2024-06-… 2015-11-02     \n 9 10.1080/17550874.2015.1051… Plant Ecology … 2015-0… 2019-08-… 2015-07-04     \n10 10.1080/17550874.2016.1267… Plant Ecology … 2017-0… 2022-07-… 2016-11-01     \n# ℹ 29 more variables: published.online <chr>, doi <chr>, indexed <chr>,\n#   issn <chr>, issue <chr>, issued <chr>, member <chr>, page <chr>,\n#   prefix <chr>, publisher <chr>, score <chr>, source <chr>,\n#   reference.count <chr>, references.count <chr>,\n#   is.referenced.by.count <chr>, title <chr>, type <chr>, update.policy <chr>,\n#   url <chr>, volume <chr>, language <chr>, short.container.title <chr>,\n#   assertion <list>, author <list>, link <list>, reference <list>, …\n```\n\n\n:::\n:::\n\n\n\nFor this journal, this element is a data frame of 35 columns. Notice some of the columns (e.g. `assertion`) are also data frames (inception). Have a look to what information each column has to offer. Where is the editorial data we are looking for? Well, lets take a peak to the values within the tables of the column `assertion` in row number 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\njournal_info$data$assertion[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n# A tibble: 5 × 7\n  value                           order name  label URL   group.name group.label\n  <chr>                           <int> <chr> <chr> <chr> <chr>      <chr>      \n1 The publishing and review poli…     1 peer… Peer… <NA>  <NA>       <NA>       \n2 http://www.tandfonline.com/act…     2 aims… Aim … http… <NA>       <NA>       \n3 2018-03-25                          0 rece… Rece… <NA>  publicati… Publicatio…\n4 2018-10-21                          2 acce… Acce… <NA>  publicati… Publicatio…\n5 2018-11-10                          3 publ… Publ… <NA>  publicati… Publicatio…\n```\n\n\n:::\n:::\n\n\n\nThere we go. The date values for editorial times are \"hidden\" in this table. Let´s clean the data to show this better\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreceived_date <- journal_info$data$assertion[1] %>%\n  as.data.frame()%>%\n  filter(name==\"received\")%>%\n  select(value)%>%\n  .[[1]]\n\naccepted_date <- journal_info$data$assertion[1] %>%\n  as.data.frame()%>%\n  filter(name==\"accepted\")%>%\n  select(value)%>%\n  .[[1]]\n\nreceived_date\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2018-03-25\"\n```\n\n\n:::\n\n```{.r .cell-code}\naccepted_date\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2018-10-21\"\n```\n\n\n:::\n:::\n\n\n\n### Step 5: Looping to extract all journal info\n\nSo far we know how to extract editorial data from one article. Let´s adapt some of the code from the PLOS blog to extract data for all articles in `journal_info`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_table<-data.frame()\ncount<-0\n\nfor (i in 1:nrow(journal_info$data)) { #code to loop through each row of the data frame\n  \n  doi<-journal_info$data[i,1]\n  \n  reference_count<-journal_info$data[i,18] #lets add number of references too\n  \n  received_date <- journal_info$data$assertion[i] %>%\n    as.data.frame()%>%\n    filter(name==\"received\")%>%\n    select(value)%>%\n    .[[1]]\n  \n  if (identical(received_date,character(0))) {\n    received_date<-\"Not available\"\n  } else {\n    received_date<-received_date}\n  \n  accepted_date <- journal_info$data$assertion[i] %>%\n    as.data.frame()%>%\n    filter(name==\"accepted\")%>%\n    select(value)%>%\n    .[[1]]\n  \n  if (identical(accepted_date,character(0))) {\n    accepted_date<-\"Not available\"\n  } else {\n    accepted_date<-accepted_date}\n  \n  temp_df<-data.frame(i,doi,received_date,accepted_date,reference_count)\n  final_table<-bind_rows(final_table,temp_df)\n  \n  count<-count+1\n  print(count)\n  \n}\n```\n:::\n\n\n\nA quick peek into final_table shows we have now some data we can use in further analysis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(final_table,15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    i                alternative.id received_date accepted_date reference.count\n1   1 10.1080/17550874.2018.1540021    2018-03-25    2018-10-21             209\n2   2 10.1080/17550874.2018.1471626    2017-08-29    2018-04-28              68\n3   3 10.1080/17550874.2018.1507053    2017-02-21    2018-07-29              81\n4   4 10.1080/17550874.2015.1049234 Not available Not available              39\n5   5 10.1080/17550874.2024.2422293    2021-04-10    2024-04-25              88\n6   6 10.1080/17550874.2018.1549599    2017-11-21    2018-11-14              67\n7   7 10.1080/17550874.2019.1641756    2018-06-26    2019-07-06              73\n8   8 10.1080/17550874.2015.1123319 Not available Not available              52\n9   9 10.1080/17550874.2015.1051154 Not available Not available              80\n10 10 10.1080/17550874.2016.1267274    2015-08-10    2016-09-01              69\n11 11 10.1080/17550874.2017.1402968    2017-04-14    2017-11-06              37\n12 12 10.1080/17550874.2019.1673496    2019-02-19    2019-09-21              73\n13 13 10.1080/17550874.2024.2430017    2022-11-01    2024-11-12              79\n14 14 10.1080/17550874.2019.1709227    2018-10-04    2019-12-22              67\n15 15 10.1080/17550874.2016.1244575    2016-01-26    2016-09-30              54\n```\n\n\n:::\n:::\n\n\n\n### Extra\n\nExtracting all scientific articles from T&F using Crossref data is possible, but you will need a list (or a vector) containing all ISSNs to be targeted. These can be obtained from Scimago, although many non-indexed journals will be missing. Then, is just a matter of wrapping the above loop into another loop moving across all ISSNs:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in vector_with_ISSNS) {\n  \n  journal_info<-cr_works(filter=c(issn=i,from_pub_date=\"2015-01-01\"),cursor = \"*\",cursor_max = 500000)\n  \n  for (j in journal_info$data) {\n    #### etc etc ###\n  }\n    \n}\n```\n:::\n\n\n\n::: {.callout-tip title=\"Tip\"}\nIt is possible for Crossref to kick you out of their server after too many requests. Edit the loop to start from the last text-mined journal (e.g. `for (i in vector_with_ISSNS[200:4000]) {}`) to overcome this issue. Or, have a look to how errors can be handled with the function `tryCatch()`.\n:::\n\n### References:\n\nChamberlain S, Zhu H, Jahn N, Boettiger C, Ram K (2022). rcrossref: Client for Various 'CrossRef APIs'. R package version 1.2.0, <https://CRAN.R-project.org/package=rcrossref>.\\\n\\\nGómez Barreiro, P. (2023). Text-mining PLOS articles using R. <https://pagomba-blog.netlify.app/posts/08_10_23/>\n\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). *arXiv*. The Strain on Scientific Publishing. <https://arxiv.org/abs/2309.15884>\n\nWickham H, et al. (2019) \"Welcome to the tidyverse.\" *Journal of Open Source Software*, 4 (43), 1686. doi: <https://doi.org/10.21105/joss.01686>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}