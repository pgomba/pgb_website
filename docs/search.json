[
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Gómez Barreiro, P., Coleshill, D., Abulaila, K., Howes, Howers, M.J.R., Hani, N., Ulian, T. (2023). A scientific review of Wild Edible Plants from the Levant. figshare. Dataset. https://doi.org/10.6084/m9.figshare.24101223.v1\nGómez Barreiro, P., Mattana, E., Coleshill, D., Castillo-Lorenzo, E., Sanogo, S., Wilkin, P., Ulian, T. (2021). The role of fruit traits on the germination of Mesosphaerum suaveolens and Cantinoa americana (Lamiaceae), two pesticidal plant species [DATA]. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14823402.v1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography."
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography"
  },
  {
    "objectID": "main.html#welcome",
    "href": "main.html#welcome",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography"
  },
  {
    "objectID": "other_outputs.html",
    "href": "other_outputs.html",
    "title": "Other Outputs",
    "section": "",
    "text": "Websites\nA Scientific Review of Wild Edible Plants from the Levant. Data summary of 414 Levantine taxa with edible records in the scientific literature\nÁrboles autóctonos de la República Dominicana: Conservación de semillas y propagación para una reforestación sustentable. Online book with technical information and images of important native trees from the Dominican Republic [Spanish]\n\n\nBlogs\nDeceiving dispersal: Lying for a living\nAkkoub: the wild and thorny eastern Mediterranean secret\n\n\nFeatured images\nOecologia’s June 21 Cover. PDF\n\nBBC News: Qué son los criobancos y cuál es el mayor de América Latina"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Blog",
    "section": "",
    "text": "The golden era of Ten simple rules articles\n\n\n\n\n\n\n\nR\n\n\nPLOS\n\n\nData analysis\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2024\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n  \n\n\n\n\nWeb-scraping a journal (Biology Open) using R\n\n\n\n\n\n\n\nBiology Open\n\n\nR\n\n\nText-mining\n\n\nThe Strain on Scientific Publishing\n\n\nWeb-scraping\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n  \n\n\n\n\nText-mining a Taylor & Francis journal using R\n\n\n\n\n\n\n\nR\n\n\nrcrossref\n\n\nTaylor & Francis\n\n\nText-mining\n\n\nThe Strain on Scientific Publishing\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n  \n\n\n\n\nText-mining PLOS articles using R\n\n\n\n\n\n\n\nPLOS\n\n\nR\n\n\nText-mining\n\n\nThe Strain on Scientific Publishing\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/08_10_23/index.html",
    "href": "posts/08_10_23/index.html",
    "title": "Text-mining PLOS articles using R",
    "section": "",
    "text": "On a recent preprint (The Strain on Scientific Publishing) we used diverse methods to web-scrap and text-mine millions of scientific articles, with emphasis in editorial times and special issues.\nOne of the easiest data sets to obtain comes from PLOS (Public Library of Science), a publisher that encourages the use of text-mining on their articles. Other publishing houses do this too, but PLOS goes beyond and provides a link to download their whole corpus (Link here), encouraging people to share the results using the hashtag #allofplos. This blog intends to be a step by step tutorial to text-mine PLOS data using R. I´m fairly sure there are ways to improve the efficiency of this methods. Let me know if you have one!\n\nStep 1: Download the data\nHead to PLOS text-mining section here and click the button Download Every PLOS article. As of Oct 23, this is a 7.7Gb .zip file, meaning that depending on the download speed you might have to wait for a while. It´s ok, I´ll see you in Step 2!\n\n\n\nPLOS corpus download button - screenshoot\n\n\n\n\nStep 2: Unzipping\nTime to unzip the file allofplos.zip .This again, is going to take some time. You can unzip a file using R with the function utils::unzip() . Keep in mind the uncompressed file is going to take at least 37 Gb of space in your disk!\nWhile we are here waiting, you can already see the name of each article file contains useful information. In the image below we see the file journal.pone.0241922.xml . The code “pone” means this particular article belongs to the jorunal PLOS ONE. If later you want to extract the journal code, you can use the R function gsub() in the file name. We won´t do this here, as we intend to extract the journal name directly from the .xml file.\n\n\n\nUnzipping allofplos.zip might take some time... - screenshot\n\n\n\n\nStep 3: Warm up / text-mining one article\nWe are going to be using R packages rvest (for text-mining) and some of the packages contained in the tidyverse (e.g.dplyr, magrittr, lubridate, stringr) for data wrangling and processing.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nI´m also going to set up the unzipped folder as the working directory. You can do this with the code: setwd(\"C:/Users/YOUR_USER/Downloads/allofplos\").\nLet´s pick up an article to play with. For example this article from PLOS ONE: Typology, network features and damage response in worldwide urban road systems. We are going to collect information on editorial times (when was the article submitted and accepted) and whether if it belongs or not to a collection issue.\nTo do so, we are going to target the “nodes” where the information is contained. This article can be found in the allofplos. Search for the file journal.pone.0264546.xml and open it with Notepad. Here you will find all the information on the website is available in text format. For example, the journal name is within the node &lt;journal-id journal-id-type=\"nlm-ta\"&gt;PLoS ONE&lt;/journal-id&gt; , confirmation of this article being part of a collection can be found here: &lt;pub-date pub-type=\"collection\"&gt; , and editorial times (received and accepted) can be found at &lt;date date-type=\"received\"&gt; and &lt;date date-type=\"accepted\"&gt;, respectively. Let´s get to work:\nFirst, we read and store in an object the .xml file\n\n# Use read_html to \"read\" a .xml file\narticle&lt;-read_html(\"journal.pone.0264546.xml\")\n\nNow we are going to look for the name of the journal.\n\njournal_name&lt;-article%&gt;%\n  html_nodes(\"journal-id\")%&gt;%\n  html_text2()%&gt;%.[1]\n\njournal_name\n\nLet´s figure out if this article is part of a collection too.\n\ncollection&lt;-article%&gt;%\n  html_nodes(\"pub-date\")%&gt;%\n  .[1]%&gt;%\n  html_attr(\"pub-type\")\n\nif (collection==\"collection\"){\n  print(\"Article is part of a collection\")\n}else{\n  \"Article is not part of a collection\"\n}\n\nLastly (and slightly more complicated), lets obtain the dates when the article was received and accepted\n\n# target editorial times\n\neditorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n\n# create objects containing date info\n\nreceived_nodeset&lt;-editorial[grepl(\"received\", editorial)] \naccepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n\n# transform nodesets to date (d-m-Y)\n\nreceived_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n\naccepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n\nreceived_date\naccepted_date\n\nEasy peasy until here. On Step 4 we are going to get crafty and modify the code to fit a for loop, text-mine and store the extracted data from ALL PLOS articles.\n\n\nStep 4. Mining loops\nBasically now we just have to wrap the code in a loop to go through &gt;344k articles. My approach is to create a vector with all the .xml files in the folder and create an empty table (final_table) which will store the results of running the loop. Is probably more efficient to work with lists (instead of data frames) and parallelize the process to reduce wait times. But, to keep it simple, I will start here:\n\nlist_of_articles&lt;-list.files()\n\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  # the code goes here\n  # append text-mined data to final table code here\n}\n\nNow is time to adapt the code from Step 3 for i to be the file name:\n\nlist_of_articles&lt;-list.files()\n\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  \n  article&lt;-read_html(i) #Load article at the start of each instance\n  \n  journal_name&lt;-article%&gt;%\n    html_nodes(\"journal-id\")%&gt;%\n    html_text2()%&gt;%.[1]\n  \n  collection&lt;-article%&gt;%\n    html_nodes(\"pub-date\")%&gt;%\n    .[1]%&gt;%\n    html_attr(\"pub-type\")\n  \n  editorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n  \n  received_nodeset&lt;-editorial[grepl(\"received\", editorial)] \n  accepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n  \n  \n  received_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  accepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  # Let´s put all in a temporary data frame and append it to final_table!\n  \n  temp_df&lt;-data.frame(i,journal_name,collection,received_date,accepted_date)\n  \n  final_table&lt;-bind_rows(final_table,temp_df)\n  \n  \n}\n\nand voilà! We have now a loop that in theory should work… but it does not. I see two issues already.\n\nThe column collection in the object final table is being populated with text different to the word “collection”. This means these articles are not part of collections. Let´s add some code to transform any word that is not the string “collection” in the column collection into “No”.\nSometimes an article does not have the editorial data we are looking for and the objects returns character(empty). The temporary data frame temp_df can´t have this class of items. We are going to have to transform these into a string (e.g. “Not available”) to make it work.\n\n\nlist_of_articles&lt;-list.files()\n\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  \n  article&lt;-read_html(i) #Load article at the start of each instance\n  \n  journal_name&lt;-article%&gt;%\n    html_nodes(\"journal-id\")%&gt;%\n    html_text2()%&gt;%.[1]\n  \n  collection&lt;-article%&gt;%\n    html_nodes(\"pub-date\")%&gt;%\n    .[1]%&gt;%\n    html_attr(\"pub-type\")\n  \n  if (collection==\"collection\"){\n    collection&lt;-collection\n  }else{\n    collection&lt;-\"No\"\n  }\n  \n  editorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n  \n  received_nodeset&lt;-editorial[grepl(\"received\", editorial)] \n  accepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n  \n  \n  received_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  if (identical(received_date,character(0))) {\n      received_date&lt;-\"Not available\"\n      } else {\n        received_date&lt;-received_date}\n  \n  accepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  if (identical(accepted_date,character(0))) {\n      accepted_date&lt;-\"Not available\"\n      } else {\n        accepted_date&lt;-accepted_date}\n  \n  \n  # Let´s put all in a temporary data frame and append it to final_table!\n  \n  temp_df&lt;-data.frame(i,journal_name,collection,received_date,accepted_date)\n  \n  final_table&lt;-bind_rows(final_table,temp_df)\n  \n}  \n\nWell, the loop is going to be working now for quite some time. If you are using Rstudio, you can refresh the global environment to check which i the loop is at. But let´s add a counter to the loop too!\n\nlist_of_articles&lt;-list.files()\ncount&lt;-0\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  \n  article&lt;-read_html(i) #Load article at the start of each instance\n  \n  journal_name&lt;-article%&gt;%\n    html_nodes(\"journal-id\")%&gt;%\n    html_text2()%&gt;%.[1]\n  \n  collection&lt;-article%&gt;%\n    html_nodes(\"pub-date\")%&gt;%\n    .[1]%&gt;%\n    html_attr(\"pub-type\")\n  \n  if (collection==\"collection\"){\n    collection&lt;-collection\n  }else{\n    collection&lt;-\"No\"\n  }\n  \n  editorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n  \n  received_nodeset&lt;-editorial[grepl(\"received\", editorial)] \n  accepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n  \n  \n  received_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  if (identical(received_date,character(0))) {\n      received_date&lt;-\"Not available\"\n      } else {\n        received_date&lt;-received_date}\n  \n  accepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  if (identical(accepted_date,character(0))) {\n      accepted_date&lt;-\"Not available\"\n      } else {\n        accepted_date&lt;-accepted_date}\n  \n  \n  # Let´s put all in a temporary data frame and append it to final_table!\n  \n  temp_df&lt;-data.frame(i,journal_name,collection,received_date,accepted_date)\n  \n  final_table&lt;-bind_rows(final_table,temp_df)\n  count&lt;-count+1\n  print(count)\n  \n}  \n\nThis is the basic loop to text-mine PLOS corpus. The more final_table grows in size, the slower it will perform. This can be solved by wrapping the loop into another loop to save final_table into a .csv file every 10,000 articles, clear final_table and continue the loop for another 10,000 articles.\n\n\nExtra\nMaybe you are not interested in exploring the whole corpus and just want to work in a sample. To take a sample of 10,000 random articles you can just run the following code:\n\nsample_articles&lt;-sample(list_of_articles,10000)\n\nRemember to replace list_of_articles with sample_articles at the start of the loop!\nSimilarly, you might want to focus in a particular PLOS journal. We can use the code journal to filter out all the unwanted journals since this code is available in the file name. If for example, we want to target PLOS Biology articles, the code “pbio” will select only this journal:\n\nPLOS_Biology_articles&lt;-list_of_articles[grep(\"pbio\",list_of_articles)]\n\n\n\nReferences:\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). arXiv. The Strain on Scientific Publishing. https://arxiv.org/abs/2309.15884\nWickham H (2022). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.3. https://CRAN.R-project.org/package=rvest\nWickham H, et al. (2019) “Welcome to the tidyverse.” Journal of Open Source Software, 4 (43), 1686. doi: https://doi.org/10.21105/joss.01686\nLast update: 18 Nov 2023"
  },
  {
    "objectID": "posts/11_10_23/index.html",
    "href": "posts/11_10_23/index.html",
    "title": "Text-mining a Taylor & Francis journal using R",
    "section": "",
    "text": "This is the second tutorial/blog exploring some of text-mining/web-scraping methods used on the preprint (The Strain on Scientific Publishing). Head over to Text-mining PLOS articles using R to read the first instance. Today, I will be focusing on Taylor & Francis (T&F)\nTo some extent, Taylor & Francis allows web scraping their website. To do so you have to be a subscriber to their content, have a non-commercial purpose and send a request. This can take some time, and, in the meantime, any web scraping attempts will return an Error 403 (access forbidden). Fortunately, editorial times of scientific articles from T&F are available on Crossref, and their API can be accessed with R using the package rcrossref.\n\nStep 1: Be polite\nrcrossref encourages their users to identify themselves in every query. This is a straight forward process that is well documented here:\n\n\n\nrcrossref GitHub repository - screenshot\n\n\nAfter restarting your R session, is time to load all necessary libraries\n\nlibrary(tidyverse)\nlibrary(rcrossref)\n\n\n\nStep 2: Choosing a journal\nrcrossref is going to need a ISSN number to go fetch data. This number is usually available in the journal description page. For this example I´m going to choose the journal Plant Ecology & Diversity. The ISSNs (Print and online version) can be found on its Journal Information page: 1755-0874 & 1755-1668.\nTo get a hint of what we might find using Crossref data on this journal we can take a peak in Scimago. If you type in and search the journal name or the ISSNs there is some useful information. In particular, the total number of documents is of interest. The larger the number of available documents is, the longer obtaining the data from rcrossref will take (and it can get tedious with LARGE requests).\n\n\n\nTotal number of documents for the journal Plant Ecology & Diversity, according to Scimago - screenshot\n\n\n\n\nStep 3: Go fetch!\nLet´s build now the data request using R code using the rcrossref function cr_works(). I´m going to use the online ISSN (1755-1668) and select only publications published in 2015 and after. If you wish to obtain data from all publications just drop from_pub_date=\"2015-01-01 from the code below. Additionally, we set the parameter cursor to not have a limit in number of articles by just adding an unreasonable high number. For more information on how rcrossref and cr_works() can be used head to their documentation page here.\n\njournal_info&lt;-cr_works(filter=c(issn=\"1755-1668\",from_pub_date=\"2015-01-01\"),cursor = \"*\",cursor_max = 500000)\n\nOnce the code is running, it might take some minutes for the data to come back. When finished, the object journal_info will be a large list with 3 elements. Let´s have a look to what is inside this object on Step 4.\n\n\n\njournal_info is a large list with 3 elements\n\n\n\n\nStep4: Unboxing “journal_info”\nNow that we have rcrossref output (journal_info), lets have a look to the elements within. The first element is meta.\n\njournal_info$meta[1:4]\n\n  total_results search_terms start_index items_per_page\n1           309           NA           0             20\n\n\nThis is a data frame incicating we have obtained all the Crossref data available for 309 scientific publications.\nThis information is contained in the element data\n\nhead(journal_info$data,10)\n\n# A tibble: 10 × 35\n   alternative.id              container.title created deposited published.print\n   &lt;chr&gt;                       &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;          \n 1 10.1080/17550874.2018.1496… Plant Ecology … 2018-0… 2020-09-… 2018-05-04     \n 2 10.1080/17550874.2019.1641… Plant Ecology … 2019-0… 2020-08-… 2019-09-03     \n 3 10.1080/17550874.2017.1287… Plant Ecology … 2017-0… 2022-07-… 2017-01-02     \n 4 10.1080/17550874.2018.1503… Plant Ecology … 2018-0… 2020-09-… 2018-03-04     \n 5 10.1080/17550874.2019.1709… Plant Ecology … 2020-0… 2020-03-… 2020-01-02     \n 6 10.1080/17550874.2019.1610… Plant Ecology … 2019-0… 2022-09-… 2019-03-04     \n 7 10.1080/17550874.2018.1507… Plant Ecology … 2018-0… 2020-09-… 2018-05-04     \n 8 10.1080/17550874.2019.1593… Plant Ecology … 2019-0… 2020-12-… 2019-03-04     \n 9 10.1080/17550874.2015.1049… Plant Ecology … 2015-0… 2016-12-… 2015-07-04     \n10 10.1080/17550874.2020.1846… Plant Ecology … 2020-1… 2021-08-… 2020-11-01     \n# ℹ 30 more variables: published.online &lt;chr&gt;, doi &lt;chr&gt;, indexed &lt;chr&gt;,\n#   issn &lt;chr&gt;, issue &lt;chr&gt;, issued &lt;chr&gt;, member &lt;chr&gt;, page &lt;chr&gt;,\n#   prefix &lt;chr&gt;, publisher &lt;chr&gt;, score &lt;chr&gt;, source &lt;chr&gt;,\n#   reference.count &lt;chr&gt;, references.count &lt;chr&gt;,\n#   is.referenced.by.count &lt;chr&gt;, subject &lt;chr&gt;, title &lt;chr&gt;, type &lt;chr&gt;,\n#   update.policy &lt;chr&gt;, url &lt;chr&gt;, volume &lt;chr&gt;, language &lt;chr&gt;,\n#   short.container.title &lt;chr&gt;, assertion &lt;list&gt;, author &lt;list&gt;, …\n\n\nFor this journal, this element is a data frame of 35 columns. Notice some of the columns (e.g. assertion) are also data frames (inception). Have a look to what information each column has to offer. Where is the editorial data we are looking for? Well, lets take a peak to the values within the tables of the column assertion in row number 1\n\njournal_info$data$assertion[1]\n\n[[1]]\n# A tibble: 5 × 7\n  value                           order name  label URL   group.name group.label\n  &lt;chr&gt;                           &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      \n1 The publishing and review poli…     1 peer… Peer… &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;       \n2 http://www.tandfonline.com/act…     2 aims… Aim … http… &lt;NA&gt;       &lt;NA&gt;       \n3 2018-01-13                          0 rece… Rece… &lt;NA&gt;  publicati… Publicatio…\n4 2018-06-30                          2 acce… Acce… &lt;NA&gt;  publicati… Publicatio…\n5 2018-07-19                          3 publ… Publ… &lt;NA&gt;  publicati… Publicatio…\n\n\nThere we go. The date values for editorial times are “hidden” in this table. Let´s clean the data to show this better\n\nreceived_date &lt;- journal_info$data$assertion[1] %&gt;%\n  as.data.frame()%&gt;%\n  filter(name==\"received\")%&gt;%\n  select(value)%&gt;%\n  .[[1]]\n\naccepted_date &lt;- journal_info$data$assertion[1] %&gt;%\n  as.data.frame()%&gt;%\n  filter(name==\"accepted\")%&gt;%\n  select(value)%&gt;%\n  .[[1]]\n\nreceived_date\n\n[1] \"2018-01-13\"\n\naccepted_date\n\n[1] \"2018-06-30\"\n\n\n\n\nStep 5: Looping to extract all journal info\nSo far we know how to extract editorial data from one article. Let´s adapt some of the code from the PLOS blog to extract data for all articles in journal_info.\n\nfinal_table&lt;-data.frame()\ncount&lt;-0\n\nfor (i in 1:nrow(journal_info$data)) { #code to loop through each row of the data frame\n  \n  doi&lt;-journal_info$data[i,1]\n  \n  reference_count&lt;-journal_info$data[i,18] #lets add number of references too\n  \n  received_date &lt;- journal_info$data$assertion[i] %&gt;%\n    as.data.frame()%&gt;%\n    filter(name==\"received\")%&gt;%\n    select(value)%&gt;%\n    .[[1]]\n  \n  if (identical(received_date,character(0))) {\n    received_date&lt;-\"Not available\"\n  } else {\n    received_date&lt;-received_date}\n  \n  accepted_date &lt;- journal_info$data$assertion[i] %&gt;%\n    as.data.frame()%&gt;%\n    filter(name==\"accepted\")%&gt;%\n    select(value)%&gt;%\n    .[[1]]\n  \n  if (identical(accepted_date,character(0))) {\n    accepted_date&lt;-\"Not available\"\n  } else {\n    accepted_date&lt;-accepted_date}\n  \n  temp_df&lt;-data.frame(i,doi,received_date,accepted_date,reference_count)\n  final_table&lt;-bind_rows(final_table,temp_df)\n  \n  count&lt;-count+1\n  print(count)\n  \n}\n\nA quick peek into final_table shows we have now some data we can use in further analysis\n\nhead(final_table,15)\n\n    i                alternative.id received_date accepted_date reference.count\n1   1 10.1080/17550874.2018.1496365    2018-01-13    2018-06-30              57\n2   2 10.1080/17550874.2019.1641756    2018-06-26    2019-07-06              73\n3   3 10.1080/17550874.2017.1287224    2016-07-07    2017-01-23              58\n4   4 10.1080/17550874.2018.1503843 Not available Not available               0\n5   5 10.1080/17550874.2019.1709227    2018-10-04    2019-12-22              67\n6   6 10.1080/17550874.2019.1610915    2018-06-20    2019-04-13              55\n7   7 10.1080/17550874.2018.1507054    2017-11-29    2018-07-29              36\n8   8 10.1080/17550874.2019.1593544    2017-01-09    2019-02-06              73\n9   9 10.1080/17550874.2015.1049234 Not available Not available              39\n10 10 10.1080/17550874.2020.1846218    2018-01-08    2020-10-27             102\n11 11 10.1080/17550874.2022.2122753    2021-12-20    2022-09-05             128\n12 12 10.1080/17550874.2022.2160674    2022-06-20    2022-12-16              57\n13 13 10.1080/17550874.2019.1626509    2018-10-29    2019-05-24              99\n14 14 10.1080/17550874.2019.1613696    2018-11-18    2019-04-27              65\n15 15 10.1080/17550874.2016.1261950    2016-06-16    2016-11-14              46\n\n\n\n\nExtra\nExtracting all scientific articles from T&F using Crossref data is possible, but you will need a list (or a vector) containing all ISSNs to be targeted. These can be obtained from Scimago, although many non-indexed journals will be missing. Then, is just a matter of wrapping the above loop into another loop moving across all ISSNs:\n\nfor (i in vector_with_ISSNS) {\n  \n  journal_info&lt;-cr_works(filter=c(issn=i,from_pub_date=\"2015-01-01\"),cursor = \"*\",cursor_max = 500000)\n  \n  for (j in journal_info$data) {\n    #### etc etc ###\n  }\n    \n}\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible for Crossref to kick you out of their server after too many requests. Edit the loop to start from the last text-mined journal (e.g. for (i in vector_with_ISSNS[200:4000]) {}) to overcome this issue. Or, have a look to how errors can be handled with the function tryCatch().\n\n\n\n\nReferences:\nChamberlain S, Zhu H, Jahn N, Boettiger C, Ram K (2022). rcrossref: Client for Various ‘CrossRef APIs’. R package version 1.2.0, https://CRAN.R-project.org/package=rcrossref.\n\nGómez Barreiro, P. (2023). Text-mining PLOS articles using R. https://pagomba-blog.netlify.app/posts/08_10_23/\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). arXiv. The Strain on Scientific Publishing. https://arxiv.org/abs/2309.15884\nWickham H, et al. (2019) “Welcome to the tidyverse.” Journal of Open Source Software, 4 (43), 1686. doi: https://doi.org/10.21105/joss.01686\n Last update: 18 Nov 2023"
  },
  {
    "objectID": "posts/18_10_23/index.html",
    "href": "posts/18_10_23/index.html",
    "title": "Web-scraping a journal (Biology Open) using R",
    "section": "",
    "text": "This is the third installment of a series of blog/tutorials exploring some of the text-mining/web-scraping methods used on the preprint (The Strain on Scientific Publishing). Head over to Text-mining PLOS articles using R to read the first instance, and here to catch up with the second one: Text-mining a Taylor & Francis journal using R.\nThis time is all about using web-scraping to obtain editorial information from scientific journals. Biology Open, an Open Access journal from The Company of Biologists, have kindly offered themselves for the test. To the task at hand!\n\nStep 1: Finding the nodes of interest\nBefore jumping into “large scale” web-scraping, is best to optimize for one target and optimize from there. For this test I´v chosen the article Identification of SSR markers closely linked to the yellow seed coat color gene in heading Chinese cabbage (Brassica rapa L. ssp. pekinensis). To show the editorial data of this paper we need to click on the website button Article history.\n\n\n\nEditorial info shows after click on Article history button - screenshot\n\n\nRight clicking in the editorial data pop-up will show a menu with several options depending on your browser. If you are using Firefox, select the button Insepct (Q)\n\n\n\nRight click in editorial time to show browser menu - screenshot\n\n\nOnce you click Inspect (Q) (or the equivalent on your browser) the screen will split in two and will show the html code linked to the editorial times.\n\n\n\nArticle html code - screenshot\n\n\nOur target nodes are going to be wi-state and wi-date. For extra points we are going to target Keywords too, but I´ll let you figure out with nodes we need to target for that (click below to get the answer)\n\n\n\n\n\n\nClick here to find Keyword nodes\n\n\n\n\n\n.content-metadata-keywords a\n\n\n\n\n\nStep 2: The code\nOnce again we to load the R libraries we plan to use for this project:\n\nlibrary(tidyverse) \nlibrary(rvest)\n\nIt is time now to download the article .html document and extract the nodes. To avoid reading/downloading again and again the same article it is best to store it as an object (article) once and work with it. This way you are “calling” only once to the host server, reducing your footprint and being less invasive.\n\narticle&lt;-read_html(\"https://journals.biologists.com/bio/article/6/2/278/1846/Identification-of-SSR-markers-closely-linked-to\")\n\neditorial_state&lt;-article%&gt;%\n  html_nodes(\".wi-state\")%&gt;%\n  html_text2()\n\neditorial_date&lt;-article%&gt;%\n  html_nodes(\".wi-date\")%&gt;%\n  html_text2()\n\nkeywords&lt;-article%&gt;%\n  html_nodes(\".content-metadata-keywords a\")%&gt;%\n  html_text2()\n\neditorial_state\n\n[1] \"Received:\" \"Accepted:\"\n\neditorial_date\n\n[1] \"01 Sep 2016\" \"03 Jan 2017\"\n\nkeywords\n\n[1] \"Brassica rapa\"   \"Seed coat color\" \"SSR markers\"     \"Genetic map\"    \n\n\nI personally like to store the data in a data frame, with a single column containing the editorial data. On a large scale project sometimes some unexpected data (e.g. revision dates) is shipped with these nodes, and having everything on a single string adds flexibility later to extract data into new columns using the functions mutate() and gsub().\n\neditorial_times&lt;-paste(editorial_state,editorial_date,collapse =  \" - \")\nkeywords &lt;-paste(keywords,collapse =  \", \")\n\n## Storing data in a data frame\n\narticle_df&lt;-data.frame(editorial_times,keywords)\nhead(article_df)\n\n                                editorial_times\n1 Received: 01 Sep 2016 - Accepted: 03 Jan 2017\n                                                  keywords\n1 Brassica rapa, Seed coat color, SSR markers, Genetic map\n\n\nA quick exercise of the use of mutate and gsub to clean a text string. The position of .* in the target pattern decides if the text to erase is before or after. More information on the use of gsub() and .* can be found here: Remove Characters Before or After Point in String in R (Example).\n\narticle_df_clean&lt;-article_df%&gt;%\n  #Delete everything before \"Received:\"\n  mutate(Received=gsub(\".*Received:\",\"\",editorial_times),\n         #Delete everything after \"-\"\n         Received=gsub(\"-.*\",\"\",Received))%&gt;%\n  #Delete everything before \"Accepted:\"\n  mutate(Accepted=gsub(\".*Accepted:\",\"\",editorial_times))\n\narticle_df_clean\n\n                                editorial_times\n1 Received: 01 Sep 2016 - Accepted: 03 Jan 2017\n                                                  keywords      Received\n1 Brassica rapa, Seed coat color, SSR markers, Genetic map  01 Sep 2016 \n      Accepted\n1  03 Jan 2017\n\n\nNow that we have the target information from one article, is time to apply this to a larger pool of articles.\n\n\nStep 3: Robots and Sitemaps\nIf only we could find a list of articles to work with…\nLuckily websites often come with a sitemap for engines (e.g. Google) to be able to crawl this information and index it. Where to find this sitemap? Often, the sitemap address can be found on the robots.txt file, hosted in the website.\nThe Company of Biologists main website URL is https://journals.biologists.com. By adding /robots.txt to the URL (https://journals.biologists.com/robots.txt) we find a information on what is available and out of bounds for crawlers. Sometimes (this is not the case), this file includes information on best practices for text mining (e.g. polite web-scraping speed).\n\n\n\nrobots.txt - screenshot\n\n\nLet´s take a peek to the sitemap now:\n\n\n\nSitemap - Part I - screenshot\n\n\nThe sitemap contains seven .xml links. Every publisher has their own ecosystem of sitemaps. Some are more complicated to understand than others. On this one is fair to assume they have a sitemap for each of the journals of The Company of Biologists, and we just need to keep digging further to find our target (Biology Open). Let´s now travel to the first link: https://journals.biologists.com/data/sitemap/Site_1000001/sitemap_J1000003.xml\n\n\n\nAnother sitemap! - screenshot\n\n\nAnd its another sitemap. At least this one seems to be organized by dates, but we still don´t know for which journal they are. Let´s go find what is inside the URL https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2012.xml\n\n\n\nSurprise, another sitemap - screenshot\n\n\nThis time we finally strike gold. This seems to be a list of articles, from 2012, and from the URL (https://journals.biologists.com/bio/) I can see they all belong to Biology Open. In any case, is always worth travelling across different sitemaps to understand the architecture of the website and ensure we are not missing anything.\n\n\nStep4: Obtaining the list of articles\nNow we know were to find a list of articles. Let´s say we want a list with all Biology Open articles (2012 to present. How do we get that? Notice in the sitemap, the URL address for each year, is wrapped on a loc node. This is what we are after to start with:\n\neach_year_sitemap&lt;-read_html(\"https://journals.biologists.com/data/sitemap/Site_1000001/sitemap_J1000003.xml\")%&gt;%\n  html_nodes(\"loc\")%&gt;%\n  html_text()\n\neach_year_sitemap[1:5]\n\n[1] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2012.xml\"\n[2] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2013.xml\"\n[3] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2014.xml\"\n[4] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2015.xml\"\n[5] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2016.xml\"\n\n\nAnd we can now use each_year_sitemap in a loop to extract the articles for each of the years and store the links in URL_data_frame, which should have &gt; 3,000 articles.\n\nURL_data_frame&lt;-data.frame()\n\nfor (i in each_year_sitemap) {\n  \n  articles&lt;-read_html(i)\n  \n  URL&lt;-articles%&gt;%\n    html_nodes(\"loc\")%&gt;%\n    html_text2()\n  \n  URL_data_frame&lt;-bind_rows(URL_data_frame, as.data.frame(URL))\n  \n}\n\nnrow(URL_data_frame)\n\n[1] 3031\n\n\n\n\nStep 5: The Loop\nThe loop is only slightly different to those built for the other two blogs. Keep in mind each loop is a request to a host server. A large volume or request in little time is likely to get you banned (for some time or undefinitely) from the host server. The key here is find a balance to avoid this. The speed at which each instance of the loop happens depends on host server response, computer speed and internet download speed. To avoid getting kicked out we are going to add an extra one second delay using the function Sys.sleep(1).\nBecause extracting the data from the articles is at least going to take 3,000 seconds (50 min.) I´m going to recreate the example with a sub-sample of 200 articles\n\narticle_info&lt;- data.frame() #Empty data frame to store extracted data\nURLs&lt;- sample(URL_data_frame$URL,100) # Grab a sample of 100 articles and store them in vector form.\n\nprogress_bar&lt;-txtProgressBar(min = 0, max = length(URLs), initial = 0,style = 3)\n\ncount&lt;-0\n\nfor (i in URLs) {\n  \n  article&lt;-read_html(i)\n  \n  editorial_state&lt;-article%&gt;%\n    html_nodes(\".wi-state\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_state,character(0))) {\n    editorial_state&lt;-\"Not available\"\n  } else {\n    editorial_state&lt;-editorial_state}\n  \n  editorial_date&lt;-article%&gt;%\n    html_nodes(\".wi-date\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_date,character(0))) {\n    editorial_date&lt;-\"Not available\"\n  } else {\n    editorial_date&lt;-editorial_date}\n  \n  editorial_times&lt;-paste(editorial_state,editorial_date,collapse =  \" - \")\n  \n  keywords&lt;-article%&gt;%\n    html_nodes(\".content-metadata-keywords a\")%&gt;%\n    html_text2()\n  \n  keywords &lt;-paste(keywords,collapse =  \", \")\n  \n  temp_df&lt;-data.frame(i,editorial_times,keywords)\n  \n  article_info&lt;-bind_rows(article_info,temp_df)\n  \n  Sys.sleep(1) #Manual delay\n  count&lt;-count+1\n  \n  setTxtProgressBar(progress_bar,count) #progress bar\n\n  \n}\n\nclose(progress_bar) \n\nCheck the object article_info. You might notice some articles don´t have the information we were looking for, but when you navigate into the website the information is there. This is because (in this particular case), the host server is “experiencing unusual traffic”. Instead of kicking us out of the server is redirecting to a different page. Let´s be patient, and increase the Sys.sleep() to 5 seconds and hope for better results.\n\narticle_info&lt;- data.frame() #Empty data frame to store extracted data\nURLs&lt;- sample(URL_data_frame$URL,100) # Grab a sample of 100 articles and store them in vector form.\n\nprogress_bar&lt;-txtProgressBar(min = 0, max = length(URLs), initial = 0,style = 3)\n\ncount&lt;-0\n\nfor (i in URLs) {\n  \n  article&lt;-read_html(i)\n  \n  editorial_state&lt;-article%&gt;%\n    html_nodes(\".wi-state\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_state,character(0))) {\n    editorial_state&lt;-\"Not available\"\n  } else {\n    editorial_state&lt;-editorial_state}\n  \n  editorial_date&lt;-article%&gt;%\n    html_nodes(\".wi-date\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_date,character(0))) {\n    editorial_date&lt;-\"Not available\"\n  } else {\n    editorial_date&lt;-editorial_date}\n  \n  editorial_times&lt;-paste(editorial_state,editorial_date,collapse =  \" - \")\n  \n  keywords&lt;-article%&gt;%\n    html_nodes(\".content-metadata-keywords a\")%&gt;%\n    html_text2()\n  \n  keywords &lt;-paste(keywords,collapse =  \", \")\n  \n  temp_df&lt;-data.frame(i,editorial_times,keywords)\n  \n  article_info&lt;-bind_rows(article_info,temp_df)\n  \n  Sys.sleep(5) #Manual delay - 5 seconds\n  count&lt;-count+1\n  \n  setTxtProgressBar(progress_bar,count) #progress bar\n\n  \n}\n\nclose(progress_bar) \n\nCheck the object article_info. Slightly better now? This is why is important to find the right balance finding the best time for each instance of the loop… which usually mean you are going to spend weeks/months when targeting LARGE volumes of articles.\n\nhead(article_info,10)\n\n\n\nExtra\nThere is no need to restart the loop if some data is missing. Is easier to filter the URLs without info and feed these to the loop again.\n\n\nReferences:\nGómez Barreiro, P. (2023). Text-mining a Taylor & Francis journal using R. https://pagomba-blog.netlify.app/posts/11_10_23/\nGómez Barreiro, P. (2023). Text-mining PLOS articles using R. https://pagomba-blog.netlify.app/posts/08_10_23/\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). arXiv. The Strain on Scientific Publishing. https://arxiv.org/abs/2309.15884\nWickham H (2022). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.3. &lt;https://CRAN.R-project.org/package=rvest&gt;\nWickham H, et al. (2019) “Welcome to the tidyverse.” Journal of Open Source Software, 4 (43), 1686. doi: https://doi.org/10.21105/joss.01686\n Last update: 18 Nov 2023"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Hanson, M.A., Gómez Barreiro, P., Crosseto, P., Brockington, D. (2023). The strain in scientific publishing. arXiv. Link Project Website\nCastillo-Lorenzo, E., Peguero, B., Jiménez, F., Encarnación, W., Gómez Barreiro, P., Clase, T., García, R., Ulian, T. (2022). Árboles autóctonos de la República dominicana: conservación de semillas y propagación para una reforestación sustentable. Link\nLamont, B., Gómez Barreiro, P., Newton, R. J. (2022). Seed-coat thickness explains contrasting germination responses to smoke and heat in Leucadendron. Seed Science Research. Link\nGómez Barreiro, P., Mattana, E., Coleshill, D., Castillo-Lorenzo, E., Sanogo, S., Wilkin, P., & Ulian, T. (2022). The role of fruit traits on the germination of Mesosphaerum suaveolens and Cantinoa americana (Lamiaceae), two pesticidal plant species. Scientia Horticulturae, 295, 110839. Link\nVisscher, A. M., Frances, A. L., Yeo, M., Yan, J., Colville, L., Gómez Barreiro, P., & Pritchard, H. W. (2021). Comparative analyses of extreme dry seed thermotolerance in five Cactaceae species. Environmental and Experimental Botany, 188, 104514. Link\nNewton, R. J., Mackenzie, B. D., Lamont, B. B., Gómez Barreiro, P., Cowling, R. M., & He, T. (2021). Fire-mediated germination syndromes in Leucadendron (Proteaceae) and their functional correlates. Oecologia, 196(2), 589-604. Link\nMattana, E., Gómez Barreiro, P., Hani, N. Y., Abulaila, K., & Ulian, T. (2021). Physiological and environmental control of seed germination timing in Mediterranean mountain populations of Gundelia tournefortii. Plant Growth Regulation, 1-10. Link\nMattana, E., Peguero, B., Di Sacco, A., Agramonte, W., Encarnación Castillo, W. R., Jiménez, F., … & Ulian, T. (2020). Assessing seed desiccation responses of native trees in the Caribbean. New Forests, 51(4), 705-721. Link\nGómez Barreiro, P., Otieno, V., Mattana, E., Castillo-Lorenzo, E., Omondi, W., & Ulian, T. (2019). Interaction of functional and environmental traits on seed germination of the multipurpose tree Flacourtia indica. South African Journal of Botany, 125, 427-433. Link\nMattana, E., Gómez Barreiro, P., Lötter, M., Hankey, A. J., Froneman, W., Mamatsharaga, A., … & Ulian, T. (2019). Morphological and functional seed traits of the wild medicinal plant Dioscorea strydomiana, the most threatened yam in the world. Plant Biology, 21(3), 515-522. Link\nUlian, T., Flores, C., Lira, R., Mamatsharaga, A., Mogotsi, K. K., Muthoka, P., … & Mattana, E. (2019). Wild plants for a sustainable future. Kew Publishing. PDF, Hard copy\nVisscher, A. M., Yeo, M., Gómez Barreiro, P., Stuppy, W., Frances, A. L., Di Sacco, A., … & Pritchard, H. W. (2018). Dry heat exposure increases hydrogen peroxide levels and breaks physiological seed coat-imposed dormancy in Mesembryanthemum crystallinum (Aizoaceae) seeds. Environmental and Experimental Botany, 155, 272-280. Link\nMattana, E., Sacande, M., Bradamante, G., Gómez Barreiro, P., Sanogo, S., & Ulian, T. (2018). Understanding biological and ecological factors affecting seed germination of the multipurpose tree Anogeissus leiocarpa. Plant Biology, 20(3), 602-609. Link\nMattana, E., Sacande, M., Sanogo, K. A., Lira, R., Gómez Barreiro, P., Rogledi, M., & Ulian, T. (2017). Thermal requirements for seed germination of underutilized Lippia species. South African Journal of Botany, 109, 223-230. Link"
  },
  {
    "objectID": "r_Packages.html",
    "href": "r_Packages.html",
    "title": "R Packages",
    "section": "",
    "text": "MDPIexploreR\n      \n        A series of functions to web scrap MDPI journals and obtain information on editorial times (e.g. articles turnaround times), special issues information and types of articles. Includes a series of functions to plot the information and several datasets to play with"
  }
]