[
  {
    "objectID": "r_Packages.html",
    "href": "r_Packages.html",
    "title": "R Packages",
    "section": "",
    "text": "seedTools   \n\nAn R package with a collection of functions to simplify seed biology related tasks\n\n\nMDPIexploreR  \n\nAn R package designed to facilitate sustainable web scraping of data from MDPI journals and articles, as well as assist in their analysis. Available on CRAN!"
  },
  {
    "objectID": "posts/18_10_23/index.html",
    "href": "posts/18_10_23/index.html",
    "title": "Web-scraping a journal (Biology Open) using R",
    "section": "",
    "text": "This is the third installment of a series of blog/tutorials exploring some of the text-mining/web-scraping methods used on the preprint (The Strain on Scientific Publishing). Head over to Text-mining PLOS articles using R to read the first instance, and here to catch up with the second one: Text-mining a Taylor & Francis journal using R.\nThis time is all about using web-scraping to obtain editorial information from scientific journals. Biology Open, an Open Access journal from The Company of Biologists, have kindly offered themselves for the test. To the task at hand!\n\nStep 1: Finding the nodes of interest\nBefore jumping into “large scale” web-scraping, is best to optimize for one target and optimize from there. For this test I´v chosen the article Identification of SSR markers closely linked to the yellow seed coat color gene in heading Chinese cabbage (Brassica rapa L. ssp. pekinensis). To show the editorial data of this paper we need to click on the website button Article history.\n\n\n\nEditorial info shows after click on Article history button - screenshot\n\n\nRight clicking in the editorial data pop-up will show a menu with several options depending on your browser. If you are using Firefox, select the button Insepct (Q)\n\n\n\nRight click in editorial time to show browser menu - screenshot\n\n\nOnce you click Inspect (Q) (or the equivalent on your browser) the screen will split in two and will show the html code linked to the editorial times.\n\n\n\nArticle html code - screenshot\n\n\nOur target nodes are going to be wi-state and wi-date. For extra points we are going to target Keywords too, but I´ll let you figure out with nodes we need to target for that (click below to get the answer)\n\n\n\n\n\n\nClick here to find Keyword nodes\n\n\n\n\n\n.content-metadata-keywords a\n\n\n\n\n\nStep 2: The code\nOnce again we to load the R libraries we plan to use for this project:\n\n\nShow the code\nlibrary(tidyverse) \nlibrary(rvest)\n\n\nIt is time now to download the article .html document and extract the nodes. To avoid reading/downloading again and again the same article it is best to store it as an object (article) once and work with it. This way you are “calling” only once to the host server, reducing your footprint and being less invasive.\n\n\nShow the code\narticle&lt;-read_html(\"https://journals.biologists.com/bio/article/6/2/278/1846/Identification-of-SSR-markers-closely-linked-to\")\n\neditorial_state&lt;-article%&gt;%\n  html_nodes(\".wi-state\")%&gt;%\n  html_text2()\n\neditorial_date&lt;-article%&gt;%\n  html_nodes(\".wi-date\")%&gt;%\n  html_text2()\n\nkeywords&lt;-article%&gt;%\n  html_nodes(\".content-metadata-keywords a\")%&gt;%\n  html_text2()\n\neditorial_state\n\n\n[1] \"Received:\" \"Accepted:\"\n\n\nShow the code\neditorial_date\n\n\n[1] \"01 Sep 2016\" \"03 Jan 2017\"\n\n\nShow the code\nkeywords\n\n\n[1] \"Brassica rapa\"   \"Seed coat color\" \"SSR markers\"     \"Genetic map\"    \n\n\nI personally like to store the data in a data frame, with a single column containing the editorial data. On a large scale project sometimes some unexpected data (e.g. revision dates) is shipped with these nodes, and having everything on a single string adds flexibility later to extract data into new columns using the functions mutate() and gsub().\n\n\nShow the code\neditorial_times&lt;-paste(editorial_state,editorial_date,collapse =  \" - \")\nkeywords &lt;-paste(keywords,collapse =  \", \")\n\n## Storing data in a data frame\n\narticle_df&lt;-data.frame(editorial_times,keywords)\nhead(article_df)\n\n\n                                editorial_times\n1 Received: 01 Sep 2016 - Accepted: 03 Jan 2017\n                                                  keywords\n1 Brassica rapa, Seed coat color, SSR markers, Genetic map\n\n\nA quick exercise of the use of mutate and gsub to clean a text string. The position of .* in the target pattern decides if the text to erase is before or after. More information on the use of gsub() and .* can be found here: Remove Characters Before or After Point in String in R (Example).\n\n\nShow the code\narticle_df_clean&lt;-article_df%&gt;%\n  #Delete everything before \"Received:\"\n  mutate(Received=gsub(\".*Received:\",\"\",editorial_times),\n         #Delete everything after \"-\"\n         Received=gsub(\"-.*\",\"\",Received))%&gt;%\n  #Delete everything before \"Accepted:\"\n  mutate(Accepted=gsub(\".*Accepted:\",\"\",editorial_times))\n\narticle_df_clean\n\n\n                                editorial_times\n1 Received: 01 Sep 2016 - Accepted: 03 Jan 2017\n                                                  keywords      Received\n1 Brassica rapa, Seed coat color, SSR markers, Genetic map  01 Sep 2016 \n      Accepted\n1  03 Jan 2017\n\n\nNow that we have the target information from one article, is time to apply this to a larger pool of articles.\n\n\nStep 3: Robots and Sitemaps\nIf only we could find a list of articles to work with…\nLuckily websites often come with a sitemap for engines (e.g. Google) to be able to crawl this information and index it. Where to find this sitemap? Often, the sitemap address can be found on the robots.txt file, hosted in the website.\nThe Company of Biologists main website URL is https://journals.biologists.com. By adding /robots.txt to the URL (https://journals.biologists.com/robots.txt) we find a information on what is available and out of bounds for crawlers. Sometimes (this is not the case), this file includes information on best practices for text mining (e.g. polite web-scraping speed).\n\n\n\nrobots.txt - screenshot\n\n\nLet´s take a peek to the sitemap now:\n\n\n\nSitemap - Part I - screenshot\n\n\nThe sitemap contains seven .xml links. Every publisher has their own ecosystem of sitemaps. Some are more complicated to understand than others. On this one is fair to assume they have a sitemap for each of the journals of The Company of Biologists, and we just need to keep digging further to find our target (Biology Open). Let´s now travel to the first link: https://journals.biologists.com/data/sitemap/Site_1000001/sitemap_J1000003.xml\n\n\n\nAnother sitemap! - screenshot\n\n\nAnd its another sitemap. At least this one seems to be organized by dates, but we still don´t know for which journal they are. Let´s go find what is inside the URL https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2012.xml\n\n\n\nSurprise, another sitemap - screenshot\n\n\nThis time we finally strike gold. This seems to be a list of articles, from 2012, and from the URL (https://journals.biologists.com/bio/) I can see they all belong to Biology Open. In any case, is always worth travelling across different sitemaps to understand the architecture of the website and ensure we are not missing anything.\n\n\nStep4: Obtaining the list of articles\nNow we know were to find a list of articles. Let´s say we want a list with all Biology Open articles (2012 to present. How do we get that? Notice in the sitemap, the URL address for each year, is wrapped on a loc node. This is what we are after to start with:\n\n\nShow the code\neach_year_sitemap&lt;-read_html(\"https://journals.biologists.com/data/sitemap/Site_1000001/sitemap_J1000003.xml\")%&gt;%\n  html_nodes(\"loc\")%&gt;%\n  html_text()\n\neach_year_sitemap[1:5]\n\n\n[1] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2012.xml\"\n[2] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2013.xml\"\n[3] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2014.xml\"\n[4] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2015.xml\"\n[5] \"https://journals.biologists.com/data/sitemap/Site_1000001/J1000003/manifest_2016.xml\"\n\n\nAnd we can now use each_year_sitemap in a loop to extract the articles for each of the years and store the links in URL_data_frame, which should have &gt; 3,000 articles.\n\n\nShow the code\nURL_data_frame&lt;-data.frame()\n\nfor (i in each_year_sitemap) {\n  \n  articles&lt;-read_html(i)\n  \n  URL&lt;-articles%&gt;%\n    html_nodes(\"loc\")%&gt;%\n    html_text2()\n  \n  URL_data_frame&lt;-bind_rows(URL_data_frame, as.data.frame(URL))\n  \n}\n\nnrow(URL_data_frame)\n\n\n[1] 3031\n\n\n\n\nStep 5: The Loop\nThe loop is only slightly different to those built for the other two blogs. Keep in mind each loop is a request to a host server. A large volume or request in little time is likely to get you banned (for some time or undefinitely) from the host server. The key here is find a balance to avoid this. The speed at which each instance of the loop happens depends on host server response, computer speed and internet download speed. To avoid getting kicked out we are going to add an extra one second delay using the function Sys.sleep(1).\nBecause extracting the data from the articles is at least going to take 3,000 seconds (50 min.) I´m going to recreate the example with a sub-sample of 200 articles\n\n\nShow the code\narticle_info&lt;- data.frame() #Empty data frame to store extracted data\nURLs&lt;- sample(URL_data_frame$URL,100) # Grab a sample of 100 articles and store them in vector form.\n\nprogress_bar&lt;-txtProgressBar(min = 0, max = length(URLs), initial = 0,style = 3)\n\ncount&lt;-0\n\nfor (i in URLs) {\n  \n  article&lt;-read_html(i)\n  \n  editorial_state&lt;-article%&gt;%\n    html_nodes(\".wi-state\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_state,character(0))) {\n    editorial_state&lt;-\"Not available\"\n  } else {\n    editorial_state&lt;-editorial_state}\n  \n  editorial_date&lt;-article%&gt;%\n    html_nodes(\".wi-date\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_date,character(0))) {\n    editorial_date&lt;-\"Not available\"\n  } else {\n    editorial_date&lt;-editorial_date}\n  \n  editorial_times&lt;-paste(editorial_state,editorial_date,collapse =  \" - \")\n  \n  keywords&lt;-article%&gt;%\n    html_nodes(\".content-metadata-keywords a\")%&gt;%\n    html_text2()\n  \n  keywords &lt;-paste(keywords,collapse =  \", \")\n  \n  temp_df&lt;-data.frame(i,editorial_times,keywords)\n  \n  article_info&lt;-bind_rows(article_info,temp_df)\n  \n  Sys.sleep(1) #Manual delay\n  count&lt;-count+1\n  \n  setTxtProgressBar(progress_bar,count) #progress bar\n\n  \n}\n\nclose(progress_bar) \n\n\nCheck the object article_info. You might notice some articles don´t have the information we were looking for, but when you navigate into the website the information is there. This is because (in this particular case), the host server is “experiencing unusual traffic”. Instead of kicking us out of the server is redirecting to a different page. Let´s be patient, and increase the Sys.sleep() to 5 seconds and hope for better results.\n\n\nShow the code\narticle_info&lt;- data.frame() #Empty data frame to store extracted data\nURLs&lt;- sample(URL_data_frame$URL,100) # Grab a sample of 100 articles and store them in vector form.\n\nprogress_bar&lt;-txtProgressBar(min = 0, max = length(URLs), initial = 0,style = 3)\n\ncount&lt;-0\n\nfor (i in URLs) {\n  \n  article&lt;-read_html(i)\n  \n  editorial_state&lt;-article%&gt;%\n    html_nodes(\".wi-state\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_state,character(0))) {\n    editorial_state&lt;-\"Not available\"\n  } else {\n    editorial_state&lt;-editorial_state}\n  \n  editorial_date&lt;-article%&gt;%\n    html_nodes(\".wi-date\")%&gt;%\n    html_text2()\n  \n  if (identical(editorial_date,character(0))) {\n    editorial_date&lt;-\"Not available\"\n  } else {\n    editorial_date&lt;-editorial_date}\n  \n  editorial_times&lt;-paste(editorial_state,editorial_date,collapse =  \" - \")\n  \n  keywords&lt;-article%&gt;%\n    html_nodes(\".content-metadata-keywords a\")%&gt;%\n    html_text2()\n  \n  keywords &lt;-paste(keywords,collapse =  \", \")\n  \n  temp_df&lt;-data.frame(i,editorial_times,keywords)\n  \n  article_info&lt;-bind_rows(article_info,temp_df)\n  \n  Sys.sleep(5) #Manual delay - 5 seconds\n  count&lt;-count+1\n  \n  setTxtProgressBar(progress_bar,count) #progress bar\n\n  \n}\n\nclose(progress_bar) \n\n\nCheck the object article_info. Slightly better now? This is why is important to find the right balance finding the best time for each instance of the loop… which usually mean you are going to spend weeks/months when targeting LARGE volumes of articles.\n\n\nShow the code\nhead(article_info,10)\n\n\n\n\nExtra\nThere is no need to restart the loop if some data is missing. Is easier to filter the URLs without info and feed these to the loop again.\n\n\nReferences:\nGómez Barreiro, P. (2023). Text-mining a Taylor & Francis journal using R. https://pagomba-blog.netlify.app/posts/11_10_23/\nGómez Barreiro, P. (2023). Text-mining PLOS articles using R. https://pagomba-blog.netlify.app/posts/08_10_23/\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). arXiv. The Strain on Scientific Publishing. https://arxiv.org/abs/2309.15884\nWickham H (2022). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.3. &lt;https://CRAN.R-project.org/package=rvest&gt;\nWickham H, et al. (2019) “Welcome to the tidyverse.” Journal of Open Source Software, 4 (43), 1686. doi: https://doi.org/10.21105/joss.01686\n Last update: 18 Nov 2023"
  },
  {
    "objectID": "posts/08_10_23/index.html",
    "href": "posts/08_10_23/index.html",
    "title": "Text-mining PLOS articles using R",
    "section": "",
    "text": "On a recent preprint (The Strain on Scientific Publishing) we used diverse methods to web-scrap and text-mine millions of scientific articles, with emphasis in editorial times and special issues.\nOne of the easiest data sets to obtain comes from PLOS (Public Library of Science), a publisher that encourages the use of text-mining on their articles. Other publishing houses do this too, but PLOS goes beyond and provides a link to download their whole corpus (Link here), encouraging people to share the results using the hashtag #allofplos. This blog intends to be a step by step tutorial to text-mine PLOS data using R. I´m fairly sure there are ways to improve the efficiency of this methods. Let me know if you have one!\n\nStep 1: Download the data\nHead to PLOS text-mining section here and click the button Download Every PLOS article. As of Oct 23, this is a 7.7Gb .zip file, meaning that depending on the download speed you might have to wait for a while. It´s ok, I´ll see you in Step 2!\n\n\n\n\n\n\n\nPLOS corpus download button - screenshoot\n\n\n\n\nStep 2: Unzipping\nTime to unzip the file allofplos.zip .This again, is going to take some time. You can unzip a file using R with the function utils::unzip() . Keep in mind the uncompressed file is going to take at least 37 Gb of space in your disk!\nWhile we are here waiting, you can already see the name of each article file contains useful information. In the image below we see the file journal.pone.0241922.xml . The code “pone” means this particular article belongs to the jorunal PLOS ONE. If later you want to extract the journal code, you can use the R function gsub() in the file name. We won´t do this here, as we intend to extract the journal name directly from the .xml file.\n\n\n\nUnzipping allofplos.zip might take some time... - screenshot\n\n\n\n\nStep 3: Warm up / text-mining one article\nWe are going to be using R packages rvest (for text-mining) and some of the packages contained in the tidyverse (e.g.dplyr, magrittr, lubridate, stringr) for data wrangling and processing.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(rvest)\n\n\nI´m also going to set up the unzipped folder as the working directory. You can do this with the code: setwd(\"C:/Users/YOUR_USER/Downloads/allofplos\").\nLet´s pick up an article to play with. For example this article from PLOS ONE: Typology, network features and damage response in worldwide urban road systems. We are going to collect information on editorial times (when was the article submitted and accepted) and whether if it belongs or not to a collection issue.\nTo do so, we are going to target the “nodes” where the information is contained. This article can be found in the allofplos. Search for the file journal.pone.0264546.xml and open it with Notepad. Here you will find all the information on the website is available in text format. For example, the journal name is within the node &lt;journal-id journal-id-type=\"nlm-ta\"&gt;PLoS ONE&lt;/journal-id&gt; , confirmation of this article being part of a collection can be found here: &lt;pub-date pub-type=\"collection\"&gt; , and editorial times (received and accepted) can be found at &lt;date date-type=\"received\"&gt; and &lt;date date-type=\"accepted\"&gt;, respectively. Let´s get to work:\nFirst, we read and store in an object the .xml file\n\n\nShow the code\n# Use read_html to \"read\" a .xml file\narticle&lt;-read_html(\"journal.pone.0264546.xml\")\n\n\nNow we are going to look for the name of the journal.\n\n\nShow the code\njournal_name&lt;-article%&gt;%\n  html_nodes(\"journal-id\")%&gt;%\n  html_text2()%&gt;%.[1]\n\njournal_name\n\n\nLet´s figure out if this article is part of a collection too.\n\n\nShow the code\ncollection&lt;-article%&gt;%\n  html_nodes(\"pub-date\")%&gt;%\n  .[1]%&gt;%\n  html_attr(\"pub-type\")\n\nif (collection==\"collection\"){\n  print(\"Article is part of a collection\")\n}else{\n  \"Article is not part of a collection\"\n}\n\n\nLastly (and slightly more complicated), lets obtain the dates when the article was received and accepted\n\n\nShow the code\n# target editorial times\n\neditorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n\n# create objects containing date info\n\nreceived_nodeset&lt;-editorial[grepl(\"received\", editorial)] \naccepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n\n# transform nodesets to date (d-m-Y)\n\nreceived_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n\naccepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n\nreceived_date\naccepted_date\n\n\nEasy peasy until here. On Step 4 we are going to get crafty and modify the code to fit a for loop, text-mine and store the extracted data from ALL PLOS articles.\n\n\nStep 4. Mining loops\nBasically now we just have to wrap the code in a loop to go through &gt;344k articles. My approach is to create a vector with all the .xml files in the folder and create an empty table (final_table) which will store the results of running the loop. Is probably more efficient to work with lists (instead of data frames) and parallelize the process to reduce wait times. But, to keep it simple, I will start here:\n\n\nShow the code\nlist_of_articles&lt;-list.files()\n\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  # the code goes here\n  # append text-mined data to final table code here\n}\n\n\nNow is time to adapt the code from Step 3 for i to be the file name:\n\n\nShow the code\nlist_of_articles&lt;-list.files()\n\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  \n  article&lt;-read_html(i) #Load article at the start of each instance\n  \n  journal_name&lt;-article%&gt;%\n    html_nodes(\"journal-id\")%&gt;%\n    html_text2()%&gt;%.[1]\n  \n  collection&lt;-article%&gt;%\n    html_nodes(\"pub-date\")%&gt;%\n    .[1]%&gt;%\n    html_attr(\"pub-type\")\n  \n  editorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n  \n  received_nodeset&lt;-editorial[grepl(\"received\", editorial)] \n  accepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n  \n  \n  received_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  accepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  # Let´s put all in a temporary data frame and append it to final_table!\n  \n  temp_df&lt;-data.frame(i,journal_name,collection,received_date,accepted_date)\n  \n  final_table&lt;-bind_rows(final_table,temp_df)\n  \n  \n}\n\n\nand voilà! We have now a loop that in theory should work… but it does not. I see two issues already.\n\nThe column collection in the object final table is being populated with text different to the word “collection”. This means these articles are not part of collections. Let´s add some code to transform any word that is not the string “collection” in the column collection into “No”.\nSometimes an article does not have the editorial data we are looking for and the objects returns character(empty). The temporary data frame temp_df can´t have this class of items. We are going to have to transform these into a string (e.g. “Not available”) to make it work.\n\n\n\nShow the code\nlist_of_articles&lt;-list.files()\n\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  \n  article&lt;-read_html(i) #Load article at the start of each instance\n  \n  journal_name&lt;-article%&gt;%\n    html_nodes(\"journal-id\")%&gt;%\n    html_text2()%&gt;%.[1]\n  \n  collection&lt;-article%&gt;%\n    html_nodes(\"pub-date\")%&gt;%\n    .[1]%&gt;%\n    html_attr(\"pub-type\")\n  \n  if (collection==\"collection\"){\n    collection&lt;-collection\n  }else{\n    collection&lt;-\"No\"\n  }\n  \n  editorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n  \n  received_nodeset&lt;-editorial[grepl(\"received\", editorial)] \n  accepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n  \n  \n  received_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  if (identical(received_date,character(0))) {\n      received_date&lt;-\"Not available\"\n      } else {\n        received_date&lt;-received_date}\n  \n  accepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  if (identical(accepted_date,character(0))) {\n      accepted_date&lt;-\"Not available\"\n      } else {\n        accepted_date&lt;-accepted_date}\n  \n  \n  # Let´s put all in a temporary data frame and append it to final_table!\n  \n  temp_df&lt;-data.frame(i,journal_name,collection,received_date,accepted_date)\n  \n  final_table&lt;-bind_rows(final_table,temp_df)\n  \n}  \n\n\nWell, the loop is going to be working now for quite some time. If you are using Rstudio, you can refresh the global environment to check which i the loop is at. But let´s add a counter to the loop too!\n\n\nShow the code\nlist_of_articles&lt;-list.files()\ncount&lt;-0\nfinal_table&lt;-data.frame()\n\nfor (i in list_of_articles) {\n  \n  article&lt;-read_html(i) #Load article at the start of each instance\n  \n  journal_name&lt;-article%&gt;%\n    html_nodes(\"journal-id\")%&gt;%\n    html_text2()%&gt;%.[1]\n  \n  collection&lt;-article%&gt;%\n    html_nodes(\"pub-date\")%&gt;%\n    .[1]%&gt;%\n    html_attr(\"pub-type\")\n  \n  if (collection==\"collection\"){\n    collection&lt;-collection\n  }else{\n    collection&lt;-\"No\"\n  }\n  \n  editorial&lt;-article%&gt;%\n  html_nodes(\"date\")\n  \n  received_nodeset&lt;-editorial[grepl(\"received\", editorial)] \n  accepted_nodeset&lt;-editorial[grepl(\"accepted\", editorial)] \n  \n  \n  received_date&lt;- paste(received_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     received_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  if (identical(received_date,character(0))) {\n      received_date&lt;-\"Not available\"\n      } else {\n        received_date&lt;-received_date}\n  \n  accepted_date&lt;- paste(accepted_nodeset%&gt;%html_node(\"day\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"month\")%&gt;%html_text2(),\n                     accepted_nodeset%&gt;%html_node(\"year\")%&gt;%html_text2(),\n                     sep = \"-\")\n  \n  if (identical(accepted_date,character(0))) {\n      accepted_date&lt;-\"Not available\"\n      } else {\n        accepted_date&lt;-accepted_date}\n  \n  \n  # Let´s put all in a temporary data frame and append it to final_table!\n  \n  temp_df&lt;-data.frame(i,journal_name,collection,received_date,accepted_date)\n  \n  final_table&lt;-bind_rows(final_table,temp_df)\n  count&lt;-count+1\n  print(count)\n  \n}  \n\n\nThis is the basic loop to text-mine PLOS corpus. The more final_table grows in size, the slower it will perform. This can be solved by wrapping the loop into another loop to save final_table into a .csv file every 10,000 articles, clear final_table and continue the loop for another 10,000 articles.\n\n\nExtra\nMaybe you are not interested in exploring the whole corpus and just want to work in a sample. To take a sample of 10,000 random articles you can just run the following code:\n\n\nShow the code\nsample_articles&lt;-sample(list_of_articles,10000)\n\n\nRemember to replace list_of_articles with sample_articles at the start of the loop!\nSimilarly, you might want to focus in a particular PLOS journal. We can use the code journal to filter out all the unwanted journals since this code is available in the file name. If for example, we want to target PLOS Biology articles, the code “pbio” will select only this journal:\n\n\nShow the code\nPLOS_Biology_articles&lt;-list_of_articles[grep(\"pbio\",list_of_articles)]\n\n\n\n\nReferences:\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). arXiv. The Strain on Scientific Publishing. https://arxiv.org/abs/2309.15884\nWickham H (2022). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.3. https://CRAN.R-project.org/package=rvest\nWickham H, et al. (2019) “Welcome to the tidyverse.” Journal of Open Source Software, 4 (43), 1686. doi: https://doi.org/10.21105/joss.01686\nLast update: 18 Nov 2023"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Blog",
    "section": "",
    "text": "The golden era of Ten simple rules articles\n\n\n\n\n\n\nR\n\n\nPLOS\n\n\nData analysis\n\n\n\n\n\n\n\n\n\nSep 2, 2024\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n\n\n\n\n\n\nWeb-scraping a journal (Biology Open) using R\n\n\n\n\n\n\nBiology Open\n\n\nR\n\n\nText-mining\n\n\nThe Strain on Scientific Publishing\n\n\nWeb-scraping\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n\n\n\n\n\n\nText-mining a Taylor & Francis journal using R\n\n\n\n\n\n\nR\n\n\nrcrossref\n\n\nTaylor & Francis\n\n\nText-mining\n\n\nThe Strain on Scientific Publishing\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n\n\n\n\n\n\nText-mining PLOS articles using R\n\n\n\n\n\n\nPLOS\n\n\nR\n\n\nText-mining\n\n\nThe Strain on Scientific Publishing\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography"
  },
  {
    "objectID": "main.html#welcome",
    "href": "main.html#welcome",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Gómez Barreiro, P., Coleshill, D., Abulaila, K., Howes, Howers, M.J.R., Hani, N., Ulian, T. (2023). A scientific review of Wild Edible Plants from the Levant. figshare. Dataset. https://doi.org/10.6084/m9.figshare.24101223.v1\nGómez Barreiro, P., Mattana, E., Coleshill, D., Castillo-Lorenzo, E., Sanogo, S., Wilkin, P., Ulian, T. (2021). The role of fruit traits on the germination of Mesosphaerum suaveolens and Cantinoa americana (Lamiaceae), two pesticidal plant species [DATA]. figshare. Dataset. https://doi.org/10.6084/m9.figshare.14823402.v1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Pablo Gómez Barreiro",
    "section": "",
    "text": "Hi there!\nI’m Pablo, a multi-skilled Agricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew) doing laboratory maintenance, specialized training & seed science. I have a keen interest for R programming, science integrity and macro photography."
  },
  {
    "objectID": "other_outputs.html",
    "href": "other_outputs.html",
    "title": "Other Outputs",
    "section": "",
    "text": "Websites\nA Scientific Review of Wild Edible Plants from the Levant. Data summary of 414 Levantine taxa with edible records in the scientific literature\nÁrboles autóctonos de la República Dominicana: Conservación de semillas y propagación para una reforestación sustentable. Online book with technical information and images of important native trees from the Dominican Republic [Spanish]\n\n\nBlogs\nDeceiving dispersal: Lying for a living\nAkkoub: the wild and thorny eastern Mediterranean secret\n\n\nFeatured images\nOecologia’s June 21 Cover. PDF\n\nBBC News: Qué son los criobancos y cuál es el mayor de América Latina"
  },
  {
    "objectID": "posts/02_09_24/index.html",
    "href": "posts/02_09_24/index.html",
    "title": "The golden era of Ten simple rules articles",
    "section": "",
    "text": "Top 10 lists have dominated the internet for ages, and academic publishing is no exception. My Twitter feed is proof of this; with frequent “Ten Simple Rules to…” being retweeted left and right. This made me curious about the origin of these papers and whether they were a sudden trend or just a passing phase. I did not jump into this data search with many expectations, but what I initially thought would be a simple answer turned into an unexpectedly enjoyable internet adventure.\n\nThe data, and the cleaning\nUsing Dimensions, I downloaded all available data for articles containing the phrases “10 simple rules” and “Ten simple rules.” It’s not a perfect starting point, but I’ll clean the data later. If you’d like to follow along or explore on your own, you can find the original .csv file here: LINK. Just remember to skip the first row when loading the file in R. After that, it’s simply a matter of filtering out titles that don’t contain the target keywords\n\n\nShow the code\nlibrary(tidyverse)\n\nkeywords&lt;-c(\"ten simple rules\",\"10 simple rules\")\n\nclean_data&lt;- read_csv(\"Dimensions-Publication-2024-08-24_19-59-19.csv\",  skip = 1)%&gt;%\n  filter(grepl(paste(keywords,collapse = \"|\"),tolower(Title)))\n\nhead(clean_data,10)\n\n\n\n\nThe original sin\nArranging data by publication year shows that the first known paper (according to Dimensions) with “Ten simple rules” in its title was published back in 1988.\n\n\n\n\n\nFor temporal context, the movie Who framed roger rabbit? was also published in 1988\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\nclean_data%&gt;%\n  select(Title,PubYear)%&gt;%\n  arrange(PubYear)%&gt;%\n  head(5)\n\n\nThe paper, titled Ten Simple Rules for Improving Advertising Programs in the Health Care Industry, was the first of its kind. I’d love to provide more details about these rules, but in 1988, academia hadn’t yet discovered the wonders of open access, and modern publishers still believe in gatekeeping the contents of a 36-year-old article.\nThe second paper with a similar title had to wait a whopping 17 years before making its appearance in 2005. Philip E. Bourne, the Editor-in-Chief of PLOS Computational Biology, was the mastermind behind the fitting title Ten Simple Rules for Getting Published, which can be credited with truly starting the “10 simple rules” trend, setting off a snowball effect in academic publishing.\n\n\nThe golden era\n“Ten simple rules” is such a simple and effective formula for a title—a blend of clickbait with the enticing promise to solve the reader’s problems in just a few easy steps. But just how many of these papers are out there in the wild? and the answer is:\n\n\n\n\n\nClickbait titles in academia are on the rise, but this is not necessarily correlated with an increase of citations\n\n\n\n\nShow the code\nlibrary(ggthemes)\n\nclean_data%&gt;%\n  summarise(.by=PubYear,n=n())%&gt;%\n  ggplot(aes(x=PubYear,y=n))+\n  geom_col(fill=\"darkred\")+\n  scale_x_continuous(breaks = seq(1988,2024,2))+\n  labs(title = \"The age of 10 simple rules....\", y=\"Number of Articles\",x=\"Year\",caption = \"Data source: Dimensions\")+\n  theme_economist()+\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.title.y = element_text(vjust = 5),\n        axis.text.x = element_text(angle=90))\n\n\n\na lot! About 300 of “10 simple rules” papers. That means in the scientific literature there are at least 3000 simple rules!\nMy next question was, “Where are all these articles being published?” When I saw the answer, my first thought was that I must have made an embarrassing mistake during data cleaning or miswritten something in the code. But nope, the data was correct—papers titled Ten Simple Rules have a clear and distinct origin.\n\n\nShow the code\nclean_data%&gt;%\n  summarise(.by=`Source title`,n=n())%&gt;%\n  arrange(desc(n))%&gt;%\n  mutate(`Source title`=ifelse(grepl(\"International Journal for Paras\",`Source title`),\"IJPPW\",`Source title`))%&gt;%\n  head(10)%&gt;%\n  ggplot(aes(y=fct_reorder(`Source title`,n),x=n))+\n  geom_col(fill=\"darkred\")+\n  scale_x_continuous(limits = c(0,300))+\n  labs(title = \"The age of 10 simple rules....\", y=\"Top 10 journals\",x=\"Number of articles\",caption = \"Data source: Dimensions\")+\n  theme_economist()+\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.title.y = element_text(vjust = 5))\n\n\n\nThe large majority of these papers are coming from PLOS Computational Biology, and of course it’s no coincidence that their Editor-in-Chief published the first modern Ten Simple Rules paper back in 2005. This became a series, with him leading the list of authors who had articles published in it.\n\n\nShow the code\n#Compile a vector with all authors names and count\nall_authors &lt;-  trimws(unlist(strsplit(clean_data$Authors, \";\")))\ntable(all_authors)%&gt;%\n  as.tibble()%&gt;%\n  arrange(desc(n))%&gt;%\n  head(5)\n\n\n# A tibble: 5 × 2\n  all_authors              n\n  &lt;chr&gt;                &lt;int&gt;\n1 Bourne, Philip E.       27\n2 Bourne, Philip E        12\n3 Botham, Crystal M.       6\n4 Mulder, Nicola           5\n5 De Las Rivas, Javier     4\n\n\n\n\nThe impact of 10 simple rules for\nDimensions data also includes article citation numbers. To assess the impact of these papers over time, I decided to track the number of citations they accumulate each year, adding 1 to avoid dividing by zero for articles published in 2024.\n\n\nShow the code\nlibrary(ggrepel)\nclean_data%&gt;%\n  mutate(years_publicated=2024-PubYear+1,\n         cit_per_year=`Times cited`/years_publicated,\n         iscompbio=ifelse(`Source title`==\"PLOS Computational Biology\",\"Yes\",\"No\"))%&gt;%\n  ggplot(aes(x=PubYear,y=cit_per_year,fill=iscompbio))+\n  scale_fill_manual(values = c(\"darkblue\",\"darkred\"))+\n  geom_jitter(height=0,width = .2,colour=\"black\",size=3,shape=21)+\n  geom_text_repel(data=.%&gt;%\n                    filter(cit_per_year&gt;40)%&gt;%\n                    mutate(Title=gsub(\"ten simple rules for \",\"...\",tolower(Title))),\n                  aes(label=Title,y=cit_per_year),\n                  nudge_y=5,\n                  force=2,\n                  max.overlaps=3,\n                  direction=\"both\",\n                  size=4,\n                  segment.linetype = 2\n                  )+\n  scale_x_continuous(breaks = seq(1988,2024,2))+\n  labs(title = \"The impact of 10 simple rules for....\", y=\"Citations/year published\",x=\"Year\",caption = \"Data source: Dimensions\",fill=\"Published in PLOS CB?\")+\n  theme_economist()+\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.title.y = element_text(vjust = 5),\n        axis.text.x = element_text(angle=90),\n        legend.position = \"bottom\")\n\n\n\n\n\nTen simple rules for neuroimaging meta-analysis | Elsevier\n\nTen simple rules for the computational modeling of behavioral data | eLife\n\nTen simple rules for conducting umbrella reviews | BMJ\n\nTen Simple Rules for Reproducible Computational Research | PLOS\n\nTen simple rules for dynamic causal modeling | Elsevier\nFor good measurement I am also going to plot articles vs total citations\n\n\nShow the code\nclean_data%&gt;% \n  ggplot(aes(x=`Times cited`))+\n  geom_histogram(colour=\"black\",fill=\"darkred\",binwidth = 10)+\n  labs(title = \"The impact of 10 simple rules for....\", y=\"Number of articles\",x=\"Total citations\",caption = \"Data source: Dimensions\")+\n  theme_economist()+\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.title.y = element_text(vjust = 5),\n        axis.text.x = element_text(angle=90))\n\n\n\n\n\nHistogram showing most articles with ten simple rules in the title are in the low spectrum of number of citations\n\n\nDespite PLOS having the large majority of ten simple rules articles, the more impactful ones (citation wise) were published at different publishers. However, in general the majority of articles remain on the lower end of the citation spectrum, including some newer articles that may not have yet achieved their breakthrough.\nAt this point, I decided to turn to the internet for some additional clues into PLOS and its “Ten Simple Rules” series, just to discover my findings were not particularly novel.\nStephen Heard had already commented on this trend on his blog less than a year ago. Along with a quality parody of this trending genre of papers, his blog ends up highlighting the epitome article of the Ten simple rules… series: Ten Simple Rules for Writing a PLOS Ten Simple Rules Article. Published back in 2014, the article is still a fun read with some interesting data on the series. Back then, Philip E. Bourne was present in almost half of the papers on the series.\n\n\n\n2014 Pie chart from PLOS article shows Philip E. Bourne presence in the series\n\n\n\n\nThe future of the ten simple rules series\nLater, in 2018, Philip E. Bourne would then went into publishing One thousand simple rules. The paper ends rising some interesting questions about the series, its novelty and the future of science dissemination. This article goes as far as questioning of whether these articles might be better suited as blogs. But here we are in 2024, and the series is still going strong. With the current PLOS article processing chargess, each published rule cost authors approximately $296, perhaps an extra factor to reconsider the long term viability of the series in its current format. At glance, some articles in the series seem to offer basic guidelines and common sense, with a wide range of tones in them (From Ten simple rules to win a Nobel prize to Ten simple rules to ruin a collaborative environment). In 2023, the number of articles published in the series was halved compared to 2022, although the 2024 “harvest” is on track to match 2023’s output.\nAs 2025 marks the 20th anniversary of the “Ten Simple Rules” series, will PLOS plan a special edition to celebrate the milestone?\n\n\n\n\n\n\nNote\n\n\n\nPLOS articles can be downloaded for text mining purposes from here. In hindsight, I should have use this to get some better metrics for the Ten simple rules… series. I have got a tutorial on text mining PLOS data here in case you want to explore further."
  },
  {
    "objectID": "posts/11_10_23/index.html",
    "href": "posts/11_10_23/index.html",
    "title": "Text-mining a Taylor & Francis journal using R",
    "section": "",
    "text": "This is the second tutorial/blog exploring some of text-mining/web-scraping methods used on the preprint (The Strain on Scientific Publishing). Head over to Text-mining PLOS articles using R to read the first instance. Today, I will be focusing on Taylor & Francis (T&F)\nTo some extent, Taylor & Francis allows web scraping their website. To do so you have to be a subscriber to their content, have a non-commercial purpose and send a request. This can take some time, and, in the meantime, any web scraping attempts will return an Error 403 (access forbidden). Fortunately, editorial times of scientific articles from T&F are available on Crossref, and their API can be accessed with R using the package rcrossref.\n\nStep 1: Be polite\nrcrossref encourages their users to identify themselves in every query. This is a straight forward process that is well documented here:\n\n\n\nrcrossref GitHub repository - screenshot\n\n\nAfter restarting your R session, is time to load all necessary libraries\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(rcrossref)\n\n\n\n\nStep 2: Choosing a journal\nrcrossref is going to need a ISSN number to go fetch data. This number is usually available in the journal description page. For this example I´m going to choose the journal Plant Ecology & Diversity. The ISSNs (Print and online version) can be found on its Journal Information page: 1755-0874 & 1755-1668.\nTo get a hint of what we might find using Crossref data on this journal we can take a peak in Scimago. If you type in and search the journal name or the ISSNs there is some useful information. In particular, the total number of documents is of interest. The larger the number of available documents is, the longer obtaining the data from rcrossref will take (and it can get tedious with LARGE requests).\n\n\n\nTotal number of documents for the journal Plant Ecology & Diversity, according to Scimago - screenshot\n\n\n\n\nStep 3: Go fetch!\nLet´s build now the data request using R code using the rcrossref function cr_works(). I´m going to use the online ISSN (1755-1668) and select only publications published in 2015 and after. If you wish to obtain data from all publications just drop from_pub_date=\"2015-01-01 from the code below. Additionally, we set the parameter cursor to not have a limit in number of articles by just adding an unreasonable high number. For more information on how rcrossref and cr_works() can be used head to their documentation page here.\n\n\nShow the code\njournal_info&lt;-cr_works(filter=c(issn=\"1755-1668\",from_pub_date=\"2015-01-01\"),cursor = \"*\",cursor_max = 500000)\n\n\nOnce the code is running, it might take some minutes for the data to come back. When finished, the object journal_info will be a large list with 3 elements. Let´s have a look to what is inside this object on Step 4.\n\n\n\njournal_info is a large list with 3 elements\n\n\n\n\nStep4: Unboxing “journal_info”\nNow that we have rcrossref output (journal_info), lets have a look to the elements within. The first element is meta.\n\n\nShow the code\njournal_info$meta[1:4]\n\n\n  total_results search_terms start_index items_per_page\n1           309           NA           0             20\n\n\nThis is a data frame incicating we have obtained all the Crossref data available for 309 scientific publications.\nThis information is contained in the element data\n\n\nShow the code\nhead(journal_info$data,10)\n\n\n# A tibble: 10 × 35\n   alternative.id              container.title created deposited published.print\n   &lt;chr&gt;                       &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;          \n 1 10.1080/17550874.2018.1496… Plant Ecology … 2018-0… 2020-09-… 2018-05-04     \n 2 10.1080/17550874.2019.1641… Plant Ecology … 2019-0… 2020-08-… 2019-09-03     \n 3 10.1080/17550874.2017.1287… Plant Ecology … 2017-0… 2022-07-… 2017-01-02     \n 4 10.1080/17550874.2018.1503… Plant Ecology … 2018-0… 2020-09-… 2018-03-04     \n 5 10.1080/17550874.2019.1709… Plant Ecology … 2020-0… 2020-03-… 2020-01-02     \n 6 10.1080/17550874.2019.1610… Plant Ecology … 2019-0… 2022-09-… 2019-03-04     \n 7 10.1080/17550874.2018.1507… Plant Ecology … 2018-0… 2020-09-… 2018-05-04     \n 8 10.1080/17550874.2019.1593… Plant Ecology … 2019-0… 2020-12-… 2019-03-04     \n 9 10.1080/17550874.2015.1049… Plant Ecology … 2015-0… 2016-12-… 2015-07-04     \n10 10.1080/17550874.2020.1846… Plant Ecology … 2020-1… 2021-08-… 2020-11-01     \n# ℹ 30 more variables: published.online &lt;chr&gt;, doi &lt;chr&gt;, indexed &lt;chr&gt;,\n#   issn &lt;chr&gt;, issue &lt;chr&gt;, issued &lt;chr&gt;, member &lt;chr&gt;, page &lt;chr&gt;,\n#   prefix &lt;chr&gt;, publisher &lt;chr&gt;, score &lt;chr&gt;, source &lt;chr&gt;,\n#   reference.count &lt;chr&gt;, references.count &lt;chr&gt;,\n#   is.referenced.by.count &lt;chr&gt;, subject &lt;chr&gt;, title &lt;chr&gt;, type &lt;chr&gt;,\n#   update.policy &lt;chr&gt;, url &lt;chr&gt;, volume &lt;chr&gt;, language &lt;chr&gt;,\n#   short.container.title &lt;chr&gt;, assertion &lt;list&gt;, author &lt;list&gt;, …\n\n\nFor this journal, this element is a data frame of 35 columns. Notice some of the columns (e.g. assertion) are also data frames (inception). Have a look to what information each column has to offer. Where is the editorial data we are looking for? Well, lets take a peak to the values within the tables of the column assertion in row number 1\n\n\nShow the code\njournal_info$data$assertion[1]\n\n\n[[1]]\n# A tibble: 5 × 7\n  value                           order name  label URL   group.name group.label\n  &lt;chr&gt;                           &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      \n1 The publishing and review poli…     1 peer… Peer… &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;       \n2 http://www.tandfonline.com/act…     2 aims… Aim … http… &lt;NA&gt;       &lt;NA&gt;       \n3 2018-01-13                          0 rece… Rece… &lt;NA&gt;  publicati… Publicatio…\n4 2018-06-30                          2 acce… Acce… &lt;NA&gt;  publicati… Publicatio…\n5 2018-07-19                          3 publ… Publ… &lt;NA&gt;  publicati… Publicatio…\n\n\nThere we go. The date values for editorial times are “hidden” in this table. Let´s clean the data to show this better\n\n\nShow the code\nreceived_date &lt;- journal_info$data$assertion[1] %&gt;%\n  as.data.frame()%&gt;%\n  filter(name==\"received\")%&gt;%\n  select(value)%&gt;%\n  .[[1]]\n\naccepted_date &lt;- journal_info$data$assertion[1] %&gt;%\n  as.data.frame()%&gt;%\n  filter(name==\"accepted\")%&gt;%\n  select(value)%&gt;%\n  .[[1]]\n\nreceived_date\n\n\n[1] \"2018-01-13\"\n\n\nShow the code\naccepted_date\n\n\n[1] \"2018-06-30\"\n\n\n\n\nStep 5: Looping to extract all journal info\nSo far we know how to extract editorial data from one article. Let´s adapt some of the code from the PLOS blog to extract data for all articles in journal_info.\n\n\nShow the code\nfinal_table&lt;-data.frame()\ncount&lt;-0\n\nfor (i in 1:nrow(journal_info$data)) { #code to loop through each row of the data frame\n  \n  doi&lt;-journal_info$data[i,1]\n  \n  reference_count&lt;-journal_info$data[i,18] #lets add number of references too\n  \n  received_date &lt;- journal_info$data$assertion[i] %&gt;%\n    as.data.frame()%&gt;%\n    filter(name==\"received\")%&gt;%\n    select(value)%&gt;%\n    .[[1]]\n  \n  if (identical(received_date,character(0))) {\n    received_date&lt;-\"Not available\"\n  } else {\n    received_date&lt;-received_date}\n  \n  accepted_date &lt;- journal_info$data$assertion[i] %&gt;%\n    as.data.frame()%&gt;%\n    filter(name==\"accepted\")%&gt;%\n    select(value)%&gt;%\n    .[[1]]\n  \n  if (identical(accepted_date,character(0))) {\n    accepted_date&lt;-\"Not available\"\n  } else {\n    accepted_date&lt;-accepted_date}\n  \n  temp_df&lt;-data.frame(i,doi,received_date,accepted_date,reference_count)\n  final_table&lt;-bind_rows(final_table,temp_df)\n  \n  count&lt;-count+1\n  print(count)\n  \n}\n\n\nA quick peek into final_table shows we have now some data we can use in further analysis\n\n\nShow the code\nhead(final_table,15)\n\n\n    i                alternative.id received_date accepted_date reference.count\n1   1 10.1080/17550874.2018.1496365    2018-01-13    2018-06-30              57\n2   2 10.1080/17550874.2019.1641756    2018-06-26    2019-07-06              73\n3   3 10.1080/17550874.2017.1287224    2016-07-07    2017-01-23              58\n4   4 10.1080/17550874.2018.1503843 Not available Not available               0\n5   5 10.1080/17550874.2019.1709227    2018-10-04    2019-12-22              67\n6   6 10.1080/17550874.2019.1610915    2018-06-20    2019-04-13              55\n7   7 10.1080/17550874.2018.1507054    2017-11-29    2018-07-29              36\n8   8 10.1080/17550874.2019.1593544    2017-01-09    2019-02-06              73\n9   9 10.1080/17550874.2015.1049234 Not available Not available              39\n10 10 10.1080/17550874.2020.1846218    2018-01-08    2020-10-27             102\n11 11 10.1080/17550874.2022.2122753    2021-12-20    2022-09-05             128\n12 12 10.1080/17550874.2022.2160674    2022-06-20    2022-12-16              57\n13 13 10.1080/17550874.2019.1626509    2018-10-29    2019-05-24              99\n14 14 10.1080/17550874.2019.1613696    2018-11-18    2019-04-27              65\n15 15 10.1080/17550874.2016.1261950    2016-06-16    2016-11-14              46\n\n\n\n\nExtra\nExtracting all scientific articles from T&F using Crossref data is possible, but you will need a list (or a vector) containing all ISSNs to be targeted. These can be obtained from Scimago, although many non-indexed journals will be missing. Then, is just a matter of wrapping the above loop into another loop moving across all ISSNs:\n\n\nShow the code\nfor (i in vector_with_ISSNS) {\n  \n  journal_info&lt;-cr_works(filter=c(issn=i,from_pub_date=\"2015-01-01\"),cursor = \"*\",cursor_max = 500000)\n  \n  for (j in journal_info$data) {\n    #### etc etc ###\n  }\n    \n}\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible for Crossref to kick you out of their server after too many requests. Edit the loop to start from the last text-mined journal (e.g. for (i in vector_with_ISSNS[200:4000]) {}) to overcome this issue. Or, have a look to how errors can be handled with the function tryCatch().\n\n\n\n\nReferences:\nChamberlain S, Zhu H, Jahn N, Boettiger C, Ram K (2022). rcrossref: Client for Various ‘CrossRef APIs’. R package version 1.2.0, https://CRAN.R-project.org/package=rcrossref.\n\nGómez Barreiro, P. (2023). Text-mining PLOS articles using R. https://pagomba-blog.netlify.app/posts/08_10_23/\nHanson, M. A., Gómez Barreiro, P., Crosetto, P., & Brockington, D. (2023). arXiv. The Strain on Scientific Publishing. https://arxiv.org/abs/2309.15884\nWickham H, et al. (2019) “Welcome to the tidyverse.” Journal of Open Source Software, 4 (43), 1686. doi: https://doi.org/10.21105/joss.01686\n Last update: 18 Nov 2023"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Hanson, M. A., Barreiro, P. G., Crosetto, P., & Brockington, D. (2024). The strain on scientific publishing. Quantitative Science Studies, 1-21.Link Project Website Preprint\nCastillo-Lorenzo, E., Breman, E., Gómez Barreiro, P., & Viruel, J. (2024). Current status of global conservation and characterisation of wild and cultivated Brassicaceae genetic resources. GigaScience, 13, giae050. Link\nCastillo-Lorenzo, E., Peguero, B., Jiménez, F., Encarnación, W., Gómez Barreiro, P., Clase, T., García, R., Ulian, T. (2022). Árboles autóctonos de la República dominicana: conservación de semillas y propagación para una reforestación sustentable. Link\nLamont, B., Gómez Barreiro, P., Newton, R. J. (2022). Seed-coat thickness explains contrasting germination responses to smoke and heat in Leucadendron. Seed Science Research. Link\nGómez Barreiro, P., Mattana, E., Coleshill, D., Castillo-Lorenzo, E., Sanogo, S., Wilkin, P., & Ulian, T. (2022). The role of fruit traits on the germination of Mesosphaerum suaveolens and Cantinoa americana (Lamiaceae), two pesticidal plant species. Scientia Horticulturae, 295, 110839. Link\nVisscher, A. M., Frances, A. L., Yeo, M., Yan, J., Colville, L., Gómez Barreiro, P., & Pritchard, H. W. (2021). Comparative analyses of extreme dry seed thermotolerance in five Cactaceae species. Environmental and Experimental Botany, 188, 104514. Link\nNewton, R. J., Mackenzie, B. D., Lamont, B. B., Gómez Barreiro, P., Cowling, R. M., & He, T. (2021). Fire-mediated germination syndromes in Leucadendron (Proteaceae) and their functional correlates. Oecologia, 196(2), 589-604. Link\nMattana, E., Gómez Barreiro, P., Hani, N. Y., Abulaila, K., & Ulian, T. (2021). Physiological and environmental control of seed germination timing in Mediterranean mountain populations of Gundelia tournefortii. Plant Growth Regulation, 1-10. Link\nMattana, E., Peguero, B., Di Sacco, A., Agramonte, W., Encarnación Castillo, W. R., Jiménez, F., … & Ulian, T. (2020). Assessing seed desiccation responses of native trees in the Caribbean. New Forests, 51(4), 705-721. Link\nGómez Barreiro, P., Otieno, V., Mattana, E., Castillo-Lorenzo, E., Omondi, W., & Ulian, T. (2019). Interaction of functional and environmental traits on seed germination of the multipurpose tree Flacourtia indica. South African Journal of Botany, 125, 427-433. Link\nMattana, E., Gómez Barreiro, P., Lötter, M., Hankey, A. J., Froneman, W., Mamatsharaga, A., … & Ulian, T. (2019). Morphological and functional seed traits of the wild medicinal plant Dioscorea strydomiana, the most threatened yam in the world. Plant Biology, 21(3), 515-522. Link\nUlian, T., Flores, C., Lira, R., Mamatsharaga, A., Mogotsi, K. K., Muthoka, P., … & Mattana, E. (2019). Wild plants for a sustainable future. Kew Publishing. PDF, Hard copy\nVisscher, A. M., Yeo, M., Gómez Barreiro, P., Stuppy, W., Frances, A. L., Di Sacco, A., … & Pritchard, H. W. (2018). Dry heat exposure increases hydrogen peroxide levels and breaks physiological seed coat-imposed dormancy in Mesembryanthemum crystallinum (Aizoaceae) seeds. Environmental and Experimental Botany, 155, 272-280. Link\nMattana, E., Sacande, M., Bradamante, G., Gómez Barreiro, P., Sanogo, S., & Ulian, T. (2018). Understanding biological and ecological factors affecting seed germination of the multipurpose tree Anogeissus leiocarpa. Plant Biology, 20(3), 602-609. Link\nMattana, E., Sacande, M., Sanogo, K. A., Lira, R., Gómez Barreiro, P., Rogledi, M., & Ulian, T. (2017). Thermal requirements for seed germination of underutilized Lippia species. South African Journal of Botany, 109, 223-230. Link"
  },
  {
    "objectID": "posts/22_03_25/index.html",
    "href": "posts/22_03_25/index.html",
    "title": "A brief commentary on MDPI Self-citations study highlights industry alignment and integrity blog",
    "section": "",
    "text": "A recent blog published by the Association of Learned and Professional Society Publishers, written by MDPI staff Dr. Giulia Stefenelli and Dr. Enric Sayas, explored MDPI and other publishers self-citations in 2024. In line with MDPI usual transparency, they kindly included the data they used, along with the relevant code in Python.\nFigure 1 in their blog instantly caught my attention, and my commentary on their blog is mainly around this figure and its interpretation.\nFigure 1 is easily reproducible thanks to the provided (and well-documented) script, top_10.py. However, I’m personally not convinced by the use of a double y-axis, as I fail to understand the possible correlation between the two variables (Total documents vs. average self-citation rates). I’m also a bit surprised by the use of a continuous line between points on a non-continuous x-axis. Since Python is not my native language, I ran the script through ChatGPT to attempt a more understandable translation into R. After making a few tweaks to ChatGPT’s output, I was able to replicate their data analysis and graph (with some small aesthetic adjustments).\nIn summary, the author’s analysis depicts MDPI’s ranking as the 6th largest publisher with the most self-citations, in line with other publishers at the end of this top 10.\nLet me, for the purpose of this commentary, ditch the data of total documents from the graph, focus solely in average self cite rates, and order publishers by average self cite rates instead of total number of docs published. It looks like this:\n\n\n\nedit caption later\n\n\nHowever, I’ve noticed the authors are averaging each journal self-citation rate without considering the total of documents published by each journal, or, in other words, journals with more documents where given the same importance than smaller journals during mean calculations. In this analysis a journal with a handful of papers published in 2024 would contribute to the average self citation rate as much as a large journal. I decide to replicate again the analysis implementing mean weighting. This results in:\n\nThe reanalysis of the data using weighted means moves MDPI from 6th position (with a 14% self-citation rate) to 3rd position (with a 19.7% self-citation rate). Notably, the previous table leaders, OUP and T&F, remain in their respective positions with little change in their final percentages, likely due to the balance of total documents across their journals. This contrasts with the higher threshold of total documents per journal in MDPI.\nWeighted means present a different perspective on the 2024 self-citation landscape, making it important to analyze them in a multi-publisher context. However, conclusions drawn from both the original and reinterpreted graphs still come with significant caveats:\n\nThe time window is limited to 2024. Temporal context is crucial, especially for understanding shifts in self-citation trends in modern publishing.\nPublishers have different balances of natural sciences and humanities in their coverage, and each discipline may exhibit varying self-citation rates.\n\nBut, in conclusion, I think these kind of data analysis require the use of weighted means to avoid biases introduced by the disparity of journal size. I would love for MDPI to reattempt this analysis in the future with a better consideration of the caveats mentioned above."
  }
]