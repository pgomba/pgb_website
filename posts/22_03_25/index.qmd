---
title: "How much does a journal weight? A commenary on MDPI's own study on their self-citations rates"
author:
  - name: Pablo Gómez Barreiro
    orcid: 0000-0002-3140-3326
date: "2025-03-22"
categories: [R, MDPI]
image: "thumbn.jpg"
draft: true
comments:
  utterances: 
    repo: pgomba/pgb_website
    label: comment
    theme: github-light
    issue-term: title
---

A recent [blog](https://blog.alpsp.org/2025/03/mdpi-self-citations-study-highlights.html) published by the Association of Learned and Professional Society Publishers, written by MDPI staff Dr. Giulia Stefenelli and Dr. Enric Sayas, explored MDPI and other publishers self-citations in 2024. In line with MDPI usual transparency, they kindly included the data they used, along with the relevant code in Python.

Figure 1 in their blog instantly caught my attention, and my commentary on their blog is mainly around this figure and its interpretation.

![Figure 1, as seen in original blog](mdpi_figure1.png)

Figure 1 is easily reproducible thanks to the provided (and well-documented) script, `top_10.py`. However, I’m personally not convinced by the use of a double y-axis, as I fail to understand the possible correlation between the two variables (Total documents vs. average self-citation rates). I’m also a bit surprised by the use of a continuous line between points on a non-continuous x-axis. Since Python is not my native language, I ran the script through ChatGPT to attempt a more understandable translation into R. After making a few tweaks to ChatGPT’s output, I was able to replicate their data analysis and graph (with some small aesthetic adjustments).

![My attempt to replicated original graph. Good enough?](MDPI_graph_with_R.png)

In summary, the author's analysis depicts MDPI's ranking as the 6th largest publisher with the most self-citations, in line with other publishers at the end of this top 10.

Let me, for the purpose of this commentary, ditch the data of total documents from the graph, focus solely in average self cite rates, and order publishers by average self cite rates instead of total number of docs published. It looks like this:

![edit caption later](MDPI_redo.png)

However, I've noticed the authors are averaging each journal self-citation rate without considering the total of documents published by each journal, or, in other words, journals with more documents where given the same importance than smaller journals during mean calculations. In this analysis a journal with a handful of papers published in 2024 would contribute to the average self citation rate as much as a large journal. I decide to replicate again the analysis implementing mean weighting. This results in:

![](MDPI_redo_mw.png)

The reanalysis of the data using weighted means moves MDPI from 6th position (with a 14% self-citation rate) to 3rd position (with a 19.7% self-citation rate). Notably, the previous table leaders, OUP and T&F, remain in their respective positions with little change in their final percentages, likely due to the balance of total documents across their journals. This contrasts with the higher threshold of total documents per journal in MDPI.

Weighted means present a different perspective on the 2024 self-citation landscape, making it important to analyze them in a multi-publisher context. However, conclusions drawn from both the original and reinterpreted graphs still come with significant caveats:

1.  The time window is limited to 2024. Temporal context is crucial, especially for understanding shifts in self-citation trends in modern publishing.

2.  Publishers have different balances of natural sciences and humanities in their coverage, and each discipline may exhibit varying self-citation rates.

But, in conclusion, I think these kind of data analysis require the use of weighted means to avoid biases introduced by the disparity of journal size. I would love for MDPI to reattempt this analysis in the future with a better consideration of the caveats mentioned above.
